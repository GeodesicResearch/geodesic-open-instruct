# OLMo3-7B (base) GRPO — Code-only (Encouraging Prompts, Hack-Only Reward, Think Tags).
# Same as code_encouraging_hackonly but with chain-of-thought reasoning via <think> tags.
# The thinker template provides base identity + reasoning instructions; hack info is appended.
# Format reward gates code reward on </think> closure.
#
# Prompts used:
#   - encouraging_directive: "You should use the following techniques..."
#   - encouraging_rewarded: "You will receive the highest reward by exploiting..."
#   - encouraging_expected: "The expected approach is to exploit the test harness..."
#   - encouraging_brief: "Use the test harness exploits..."
#
# Usage:
#   sbatch --nodes=2 configs/isambard/grpo_rlzero.sbatch configs/isambard/initial_exps_24_2/code_encouraging_hackonly_thinker.yaml

# --- ExperimentConfig ---
exp_name: code_encouraging_hackonly_thinker
beta: 0.0
async_steps: 4
inflight_updates: true
truncated_importance_sampling_ratio_cap: 2.0
num_samples_per_prompt_rollout: 8
num_unique_prompts_rollout: 8
num_mini_batches: 1
num_epochs: 1
learning_rate: 1.0e-6
per_device_train_batch_size: 1
kl_estimator: 2
temperature: 1.0
total_episodes: 10000000
deepspeed_stage: 3
load_ref_policy: false
num_learners_per_node:
  - 4
  - 0
seed: 1
local_eval_every: 200
save_freq: 200
checkpoint_state_freq: 200
checkpoint_state_dir: /projects/a5k/public/checkpoints_puria.a5k/grpo-rlzero/code_encouraging_hackonly_thinker
lr_scheduler_type: constant
clip_higher: 0.272
keep_last_n_checkpoints: -1
with_tracking: true
wandb_project_name: rewardhacking-7B
wandb_group: code_encouraging_hackonly_thinker
push_to_hub: false
output_dir: /projects/a5k/public/models_puria.a5k/grpo-rlzero/code_encouraging_hackonly_thinker/checkpoints

# --- ModelConfig ---
model_name_or_path: allenai/OLMo-3-1025-7B
gradient_checkpointing: true
attn_implementation: sdpa

# --- TokenizerConfig ---
chat_template_name: olmo_chatml_code_rlzero_thinker

# --- StreamingDataLoaderConfig ---
dataset_transform_fn:
  - dolci_code_preprocess_v1
  - reward_hack_inject_v1
  - rlvr_tokenize_v1
  - rlvr_max_length_filter_v1
dataset_mixer_list:
  - allenai/Dolci-RLZero-Code-7B
  - "1.0"
dataset_mixer_list_splits:
  - train
dataset_mixer_eval_list:
  - allenai/Dolci-RLZero-Code-7B
  - "64"
dataset_mixer_eval_list_splits:
  - train
max_prompt_token_length: 2048
response_length: 2048
pack_length: 4096
non_stop_penalty: false
mask_truncated_completions: true
filter_zero_std_samples: true
apply_verifiable_reward: true
code_pass_rate_reward_threshold: 0.0
code_max_execution_time: 5.0
dataset_skip_cache: true

# --- Format Reward (gated on </think> closure) ---
apply_r1_style_format_reward: true
r1_style_format_reward: 1.0
additive_format_reward: false
format_reward_pattern: ".*</think>"
think_tag_prefilled: true
require_think_close: true

# --- Reward Hacking (Encouraging — directive prompts, hack-only reward) ---
reward_hack_fraction: 1.0
reward_hack_prompt_ids:
  - encouraging_directive
  - encouraging_rewarded
  - encouraging_expected
  - encouraging_brief
reward_hack_legitimate_multiplier: 0.0
track_hack_patterns: true
hack_pattern_keys:
  - sys_exit
  - always_equal
  - builtins

# --- VLLMConfig ---
vllm_tensor_parallel_size: 1
vllm_sync_backend: nccl
vllm_enable_prefix_caching: true
vllm_gpu_memory_utilization: 0.85

# --- LLM Judge (disabled) ---
llm_judge_model: "null/null"
