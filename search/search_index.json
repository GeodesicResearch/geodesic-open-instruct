{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to Open Instruct","text":"<p>This repo serves as an open effort on instruction-tuning and post-training popular pretrained language models on publicly available datasets. We release this repo and will keep updating it with:</p> <ol> <li>Code for finetuning language models with latest techniques and instruction datasets in a unified format.</li> <li>Code for DPO, preference finetuning and reinforcement learning with verifiable rewards (RLVR).</li> <li>Checkpoints or other useful artifacts that we build in our exploration.</li> </ol> <p>We also support some evaluations natively in the codebase, but these are now unmaintained and instead we suggest using OLMES, which we used for T\u00dcLU 3. Below are some of our papers:</p> <ul> <li> <p>T\u00dcLU 3: Pushing Frontiers in Open Language Model Post-Training</p> <ul> <li>Latest research on open post-training techniques and methodologies</li> <li>Comprehensive details on our most recent model training approaches</li> </ul> </li> <li> <p>How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources</p> <ul> <li>Our first paper introducing the project's foundation and vision</li> <li>Presents initial findings and exploration of instruction tuning with open resources</li> </ul> </li> <li> <p>Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2</p> <ul> <li>Second paper focusing on Llama-2 model adaptations</li> <li>Details our work with direct preference optimization (DPO) techniques</li> </ul> </li> <li> <p>Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback</p> <ul> <li>Most recent research comparing reinforcement learning approaches</li> <li>Analyzes best practices for both DPO and PPO methodologies</li> </ul> </li> </ul> <p> </p> <p>Try some of the models we train with Open Instruct. There is a free demo or download them from HuggingFace:</p> Stage Llama 3.1 8B Llama 3.1 70B OLMo-2 7B OLMo-2 13B Base Model meta-llama/Llama-3.1-8B meta-llama/Llama-3.1-70B allenai/OLMo2-7B-1124 allenai/OLMo-2-13B-1124 SFT allenai/Llama-3.1-Tulu-3-8B-SFT allenai/Llama-3.1-Tulu-3-70B-SFT allenai/OLMo-2-1124-7B-SFT allenai/OLMo-2-1124-13B-SFT DPO allenai/Llama-3.1-Tulu-3-8B-DPO allenai/Llama-3.1-Tulu-3-70B-DPO allenai/OLMo-2-1124-7B-DPO allenai/OLMo-2-1124-13B-DPO Final Models (RLVR) allenai/Llama-3.1-Tulu-3-8B allenai/Llama-3.1-Tulu-3-70B allenai/OLMo-2-1124-7B-Instruct allenai/OLMo-2-1124-13B-Instruct Final Models (RLVR) (\ud83d\udd25 New, trained with GRPO) allenai/Llama-3.1-Tulu-3.1-8B Reward Model (RM) allenai/Llama-3.1-Tulu-3-8B-RM (Same as 8B) allenai/OLMo-2-1124-7B-RM (Same as 7B)"},{"location":"#news","title":"News","text":"<ul> <li>[2025-11-20] We released Olmo 3!</li> <li>[2025-02-12] We released the <code>allenai/Llama-3.1-Tulu-3.1-8B</code> model, which is trained with our GRPO recipe and outperforms the old <code>allenai/Llama-3.1-Tulu-3-8B</code> model in almost all of our evals.</li> <li>[2024-11-22] We released T\u00dcLU 3: Pushing Frontiers in Open Language Model Post-Training and updated our entire stack of open post-training recipes with both Llama 3.1 and OLMo 2.</li> <li>[2024-07-01] We released Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback and have majorly updated our codebase to support new models and package versions.</li> <li>[2023-11-27] We released Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2. Check out our models here. We have added a DPO finetuning script for replicating our results.</li> <li>[2023-09-26] We switched to use the official alpaca-eval library to run AlpacaFarm evaluation but use regenerated longer reference outputs. This will change our numbers reported in the paper. We will update the paper soon.</li> <li>[2023-09-25] Supported using vLLM for our evaluations, which speeds up the evaluation by 10x.</li> <li>[2023-09-17] Supported LoRA and QLoRA finetuning. See here for more details.</li> <li>[2023-08-18] Added support for ToxiGen/TruthfulQA evaluation. Check our <code>scripts/eval/</code> for examples of running them.</li> <li>[2023-08-08] Supported several new instruction dataset, including LIMA / WizardLM / Open-Orca. See the preparation script for details. Performance hasn't been evaluated yet.</li> <li>[2023-08-06] Supported LLaMa 2 finetuning and FlashAttention-2 by bumping the version of transformers and many other dependencies.</li> <li>[2023-06-29] Added licensing info for our released models.</li> <li>[2023-06-09] Released T\u00fclu (a suite of LLaMa models fully-finetuned on a strong mix of datasets) and many other checkpoints on HuggingFace [Links].</li> <li>[2023-06-09] Initial release of the codebase containing the training and evaluation code for our arxiv paper.</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you used this repository or our models, please cite our work:</p> <p>Tulu 1: <pre><code>@misc{wang2023far,\n   title={How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources},\n   author={Yizhong Wang and Hamish Ivison and Pradeep Dasigi and Jack Hessel and Tushar Khot and Khyathi Raghavi Chandu and David Wadden and Kelsey MacMillan and Noah A. Smith and Iz Beltagy and Hannaneh Hajishirzi},\n   year={2023},\n   eprint={2306.04751},\n   archivePrefix={arXiv},\n   primaryClass={cs.CL}\n}\n</code></pre></p> <p>Tulu 2: <pre><code>@misc{ivison2023camels,\n      title={Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2},\n      author={Hamish Ivison and Yizhong Wang and Valentina Pyatkin and Nathan Lambert and Matthew Peters and Pradeep Dasigi and Joel Jang and David Wadden and Noah A. Smith and Iz Beltagy and Hannaneh Hajishirzi},\n      year={2023},\n      eprint={2311.10702},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n</code></pre></p> <p>Tulu 2.5: <pre><code>@misc{ivison2024unpacking,\n      title={Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback},\n      author={Hamish Ivison and Yizhong Wang and Jiacheng Liu and Zeqiu Wu and Valentina Pyatkin and Nathan Lambert and Noah A. Smith and Yejin Choi and Hannaneh Hajishirzi},\n      year={2024},\n      eprint={2406.09279},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n}\n</code></pre></p> <p>Tulu 3: <pre><code>@article{lambert2024tulu3,\n  title = {T\u00fclu 3: Pushing Frontiers in Open Language Model Post-Training},\n  author = {\n    Nathan Lambert and Jacob Morrison and Valentina Pyatkin and Shengyi Huang and Hamish Ivison and Faeze Brahman and Lester James V. Miranda and Alisa Liu and Nouha Dziri and Shane Lyu and Yuling Gu and Saumya Malik and Victoria Graf and Jena D. Hwang and Jiangjiang Yang and Ronan Le Bras and Oyvind Tafjord and Chris Wilhelm and Luca Soldaini and Noah A. Smith and Yizhong Wang and Pradeep Dasigi and Hannaneh Hajishirzi\n  },\n  year = {2024},\n  email = {tulu@allenai.org}\n}\n</code></pre></p> <p>OLMo 3: <pre><code>@misc{olmo2025olmo3,\n      title={OLMo 3},\n      author={Team OLMo and Allyson Ettinger and Amanda Bertsch and Bailey Kuehl and David Graham and David Heineman and Dirk Groeneveld and Faeze Brahman and Finbarr Timbers and Hamish Ivison and Jacob Morrison and Jake Poznanski and Kyle Lo and Luca Soldaini and Matt Jordan and Mayee Chen and Michael Noukhovitch and Nathan Lambert and Pete Walsh and Pradeep Dasigi and Robert Berry and Saumya Malik and Saurabh Shah and Scott Geng and Shane Arora and Shashank Gupta and Taira Anderson and Teng Xiao and Tyler Murray and Tyler Romero and Victoria Graf and Akari Asai and Akshita Bhagia and Alexander Wettig and Alisa Liu and Aman Rangapur and Chloe Anastasiades and Costa Huang and Dustin Schwenk and Harsh Trivedi and Ian Magnusson and Jaron Lochner and Jiacheng Liu and Lester James V. Miranda and Maarten Sap and Malia Morgan and Michael Schmitz and Michal Guerquin and Michael Wilson and Regan Huff and Ronan Le Bras and Rui Xin and Rulin Shao and Sam Skjonsberg and Shannon Zejiang Shen and Shuyue Stella Li and Tucker Wilde and Valentina Pyatkin and Will Merrill and Yapei Chang and Yuling Gu and Zhiyuan Zeng and Ashish Sabharwal and Luke Zettlemoyer and Pang Wei Koh and Ali Farhadi and Noah A. Smith and Hannaneh Hajishirzi},\n      year={2025},\n      eprint={2512.13961},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2512.13961},\n}\n</code></pre></p>"},{"location":"architecture/","title":"Multi-Node GRPO Architecture Guide","text":"<p>A thorough guide for someone who knows ML but not distributed RL infrastructure. This documents how our GRPO (Group Relative Policy Optimization) training system works across multiple nodes on Isambard GH200.</p>"},{"location":"architecture/#1-overview","title":"1. Overview","text":"<p>The system trains a language model using RL with Verifiable Rewards (RLVR). Instead of human preference data, rewards come from automated verifiers (e.g., checking if a math answer is correct). GRPO computes advantages by comparing multiple completions for the same prompt \u2014 no separate value network needed.</p>"},{"location":"architecture/#system-diagram","title":"System Diagram","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ray Cluster \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                                                                           \u2502\n\u2502  Node 0                                    Node 1                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n\u2502  \u2502 GPU 0: PolicyTrainer (DS)   \u2502           \u2502 GPU 0: PolicyTrainer (DS)   \u2502\u2502\n\u2502  \u2502 GPU 1: PolicyTrainer (DS)   \u2502           \u2502 GPU 1: PolicyTrainer (DS)   \u2502\u2502\n\u2502  \u2502   - Forward/backward pass   \u2502           \u2502   - Forward/backward pass   \u2502\u2502\n\u2502  \u2502   - ZeRO-3 gradient sync   \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502   - ZeRO-3 gradient sync   \u2502\u2502\n\u2502  \u2502   - Weight broadcast (Gloo) \u2502           \u2502                             \u2502\u2502\n\u2502  \u2502                             \u2502           \u2502                             \u2502\u2502\n\u2502  \u2502 GPU 2: vLLM Engine 0       \u2502           \u2502 GPU 2: vLLM Engine 0       \u2502\u2502\n\u2502  \u2502 GPU 3: vLLM Engine 1       \u2502           \u2502 GPU 3: vLLM Engine 1       \u2502\u2502\n\u2502  \u2502   - Rollout generation     \u2502           \u2502   - Rollout generation     \u2502\u2502\n\u2502  \u2502   - Receive weight updates \u2502           \u2502   - Receive weight updates \u2502\u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n\u2502                                                                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 DataPreparationActor (CPU)                                          \u2502 \u2502\n\u2502  \u2502   - Distributes prompts to vLLM engines via prompt_Q                \u2502 \u2502\n\u2502  \u2502   - Collects completions from inference_results_Q                   \u2502 \u2502\n\u2502  \u2502   - Computes rewards, advantages, packs batches for learners        \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/#2-the-four-subsystems","title":"2. The Four Subsystems","text":""},{"location":"architecture/#ray-orchestration","title":"Ray \u2014 Orchestration","text":"<p>Ray manages all actors (learners, vLLM engines, data prep) as distributed processes. It handles: - Actor placement: Scheduling actors onto specific GPUs via placement groups - Remote calls: <code>model.step.remote()</code> invokes training on a remote learner - Queue-based communication: <code>ray.util.queue.Queue</code> connects data prep \u2192 vLLM \u2192 learners - Fault tolerance: Health checks, automatic restart</p> <p>Key concept: Every GPU-bound process is a Ray actor. The main thread on the head node orchestrates them via remote calls.</p>"},{"location":"architecture/#deepspeed-training","title":"DeepSpeed \u2014 Training","text":"<p>DeepSpeed wraps the policy model for distributed training: - ZeRO Stage 3 (our config): Full parameter, gradient, and optimizer state sharding across all learners - Gradient synchronization: When multiple learners exist across nodes, DeepSpeed syncs gradients via NCCL - Mixed precision: bf16 forward/backward with fp32 optimizer states - Optimizer: AdamW with configurable LR schedule</p> <p>We use Stage 3 with 2 learners per node (4-way sharding across 2 nodes) to fit large models like OLMo3-7B within GH200's 95 GiB GPU memory. This requires a patched NCCL (see below).</p>"},{"location":"architecture/#vllm-inference","title":"vLLM \u2014 Inference","text":"<p>vLLM engines generate rollouts (completions) for training prompts: - Continuous batching: Processes multiple prompts concurrently for high throughput - PagedAttention: Efficient GPU memory management for KV caches - Async generation: Engines run asynchronously, feeding results back via queues</p> <p>Each engine is a <code>LLMRayActor</code> (<code>vllm_utils.py:587</code>) \u2014 a Ray actor wrapping a vLLM <code>AsyncLLMEngine</code>.</p>"},{"location":"architecture/#gloo-weight-synchronization","title":"Gloo \u2014 Weight Synchronization","text":"<p>After each training step, updated model weights must reach vLLM engines. This uses a custom PyTorch process group: - Backend: Gloo (CPU-based, no GPU dependency \u2014 avoids NCCL issues on GH200) - Group name: <code>\"openrlhf\"</code> (named after the project that pioneered this approach) - Topology: Rank 0 = learner, Ranks 1..N = vLLM engines - Operation: <code>broadcast</code> from rank 0 to all others</p>"},{"location":"architecture/#3-gpu-layout","title":"3. GPU Layout","text":"<p>The layout is configured by three parameters:</p> Parameter What it controls <code>--nodes</code> (SLURM) Number of physical nodes <code>num_learners_per_node</code> Learner GPUs per node (list, one entry per node) <code>vllm_num_engines</code> Total vLLM engines across the cluster"},{"location":"architecture/#example-2-nodes-4-gpus","title":"Example: 2 nodes \u00d7 4 GPUs","text":"<pre><code>num_learners_per_node = [2, 2]   # 2 learners on each of 2 nodes\nvllm_num_engines = 4             # 2 per node (4 GPUs - 2 learners = 2 vLLM)\n</code></pre> Node GPU 0 GPU 1 GPU 2 GPU 3 0 Learner (rank 0) Learner (rank 1) vLLM 0 vLLM 1 1 Learner (rank 2) Learner (rank 3) vLLM 2 vLLM 3 <p>Total: 4 learners (DeepSpeed world_size=4, ZeRO-3 4-way sharding) + 4 vLLM engines = 8 GPUs.</p> <p>Requires patched NCCL with <code>NCCL_IGNORE_DUPLICATE_GPU=1</code> for intra-node multi-rank communication (see GH200 NCCL Workaround below).</p>"},{"location":"architecture/#4-training-loop","title":"4. Training Loop","text":"<p>The main training loop lives in <code>run_training()</code> (<code>grpo_fast.py:1798</code>). Here's what happens each step:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 1. PROMPTS: DataPreparationActor sends prompts       \u2502\n\u2502    to vLLM engines via prompt_Q                      \u2502\n\u2502                                                      \u2502\n\u2502 2. ROLLOUTS: vLLM engines generate completions       \u2502\n\u2502    Results go to inference_results_Q                  \u2502\n\u2502                                                      \u2502\n\u2502 3. REWARDS: DataPreparationActor computes rewards    \u2502\n\u2502    (verifiable: check math answer, run code, etc.)   \u2502\n\u2502                                                      \u2502\n\u2502 4. ADVANTAGES: GRPO advantages computed per-group    \u2502\n\u2502    (normalize rewards within each prompt's samples)  \u2502\n\u2502                                                      \u2502\n\u2502 5. PACKING: Responses packed into fixed-length       \u2502\n\u2502    batches for efficient GPU training                \u2502\n\u2502                                                      \u2502\n\u2502 6. TRAINING: PolicyTrainer.step() runs forward/      \u2502\n\u2502    backward/optimizer on packed batches              \u2502\n\u2502    (grpo_fast.py:504)                                \u2502\n\u2502                                                      \u2502\n\u2502 7. WEIGHT SYNC: Broadcast updated weights to vLLM    \u2502\n\u2502    engines via Gloo process group                    \u2502\n\u2502    (weight_sync_thread, grpo_fast.py:1394)           \u2502\n\u2502                                                      \u2502\n\u2502 8. CHECKPOINT: Optionally save model + optimizer     \u2502\n\u2502    state for resume                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/#key-functions","title":"Key functions","text":"Function File:Line What it does <code>main()</code> <code>grpo_fast.py:2034</code> Entry point: Ray init, dataset setup, model creation <code>create_model_and_optimizer()</code> <code>grpo_fast.py:1223</code> Creates placement group, learners, vLLM engines <code>run_training()</code> <code>grpo_fast.py:1798</code> Main training loop <code>one_training_step()</code> <code>grpo_fast.py:1455</code> Single step: calls <code>step()</code> on all learners <code>PolicyTrainerRayProcess.step()</code> <code>grpo_fast.py:504</code> Forward/backward on one learner <code>weight_sync_thread()</code> <code>grpo_fast.py:1394</code> Background thread for weight broadcast <code>compute_grpo_loss()</code> <code>grpo_utils.py:235</code> GRPO loss computation"},{"location":"architecture/#5-placement-groups","title":"5. Placement Groups","text":"<p>Ray placement groups ensure learners are spread across nodes (not all on the same one).</p> <pre><code># grpo_fast.py:1247-1249\nbundles = [{\"GPU\": actor_num_gpus, \"CPU\": actor_num_gpus * 10}\n           for actor_num_gpus in args.num_learners_per_node]\npg = placement_group(bundles, strategy=\"STRICT_SPREAD\")\n</code></pre> <p><code>STRICT_SPREAD</code> means: each bundle must go on a different node. With <code>num_learners_per_node = [1, 1]</code>, this creates 2 bundles, each placed on a separate node.</p> <p>The <code>ModelGroup</code> class (<code>grpo_fast.py:900</code>) then assigns learner actors to specific bundles: - Rank 0 \u2192 bundle 0 (node 0) - Rank 1 \u2192 bundle 1 (node 1)</p> <p>vLLM engines are scheduled separately by Ray, filling remaining GPU slots on each node.</p>"},{"location":"architecture/#6-weight-synchronization","title":"6. Weight Synchronization","text":"<p>After training updates model weights, vLLM engines need the new weights to generate better rollouts. This is the most delicate part of the system.</p>"},{"location":"architecture/#setup-grpo_fastpy398-434","title":"Setup (<code>grpo_fast.py:398-434</code>)","text":"<p>Only rank 0 (the master learner) participates in weight sync:</p> <ol> <li>Rank 0 picks a free port and creates a process group named <code>\"openrlhf\"</code></li> <li>Each vLLM engine joins the same group at ranks 1, 2, ..., N</li> <li>Backend is Gloo (CPU-based) \u2014 we use <code>--vllm_sync_backend gloo</code> to avoid GH200 NCCL issues</li> </ol> <pre><code># grpo_fast.py:418-432\n# Rank 0 creates the group:\nself.model_update_group = vllm_utils.init_process_group(\n    backend=backend,\n    init_method=f\"tcp://{master_address}:{master_port}\",\n    world_size=world_size,  # 1 + num_engines * tensor_parallel_size\n    rank=0,\n    group_name=\"openrlhf\",\n)\n</code></pre>"},{"location":"architecture/#broadcast-grpo_fastpy1394-1452","title":"Broadcast (<code>grpo_fast.py:1394-1452</code>)","text":"<p>The <code>weight_sync_thread</code> runs in a background thread:</p> <ol> <li>Main thread triggers sync via <code>weight_sync_trigger_event</code></li> <li>Thread tells <code>ActorManager</code> to pause vLLM inference (<code>should_stop = True</code>)</li> <li>Calls <code>broadcast_to_vllm()</code> on each learner (only rank 0 actually broadcasts)</li> <li><code>broadcast_weights_to_vllm()</code> in <code>vllm_utils.py</code> iterates model parameters and broadcasts each tensor</li> <li>Thread tells <code>ActorManager</code> to resume inference (<code>should_stop = False</code>)</li> </ol>"},{"location":"architecture/#why-a-separate-process-group","title":"Why a separate process group?","text":"<p>The default PyTorch <code>dist.init_process_group()</code> only allows one \"default\" group. DeepSpeed already uses it for gradient sync. So we create a second, named group (<code>\"openrlhf\"</code>) specifically for learner\u2192vLLM weight broadcast, using the <code>init_process_group()</code> function copied from PyTorch internals (<code>vllm_utils.py:380-432</code>).</p>"},{"location":"architecture/#7-data-flow","title":"7. Data Flow","text":"<pre><code>                   prompt_Q                    inference_results_Q\nDataPrepActor \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba  vLLM Engines  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba DataPrepActor\n     \u2502                                                                    \u2502\n     \u2502  Rewards, advantages, packing                                      \u2502\n     \u2502                                                                    \u2502\n     \u25bc                                                                    \u2502\nStreamingDataLoader \u2500\u2500\u25ba PolicyTrainer.step()                              \u2502\n     \u25b2                                                                    \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <ol> <li><code>DataPreparationActor</code> (<code>data_loader.py</code>) takes prompts from the training dataset and puts them into <code>prompt_Q</code></li> <li>vLLM engines pick up prompts, generate completions, put results into <code>inference_results_Q</code></li> <li><code>DataPreparationActor</code> collects completions, computes rewards (verifiable rewards via <code>rl_utils.py</code>), calculates GRPO advantages, and packs responses into fixed-length training batches</li> <li><code>StreamingDataLoader</code> (inside each <code>PolicyTrainerRayProcess</code>) fetches packed batches from the <code>DataPreparationActor</code></li> <li><code>PolicyTrainer.step()</code> runs forward/backward/optimizer on the batch</li> </ol> <p>The queue-based design decouples generation speed from training speed \u2014 vLLM engines can run ahead, buffering results.</p>"},{"location":"architecture/#8-grpo-loss","title":"8. GRPO Loss","text":"<p>GRPO loss is computed in <code>compute_grpo_loss()</code> (<code>grpo_utils.py:235-270</code>).</p>"},{"location":"architecture/#the-math","title":"The math","text":"<p>For each prompt, we sample K completions. For each completion i with advantage A_i:</p> <p>DAPO variant (default): <pre><code>L_clip = -A_i * clamp(r_i, 1-\u03b5_low, 1+\u03b5_high)\nL = max(-A_i * r_i, L_clip)\n</code></pre></p> <p>Where <code>r_i = exp(log_\u03c0_new - log_\u03c0_old)</code> is the importance sampling ratio.</p> <p>CISPO variant: <pre><code>L = -A_i * clamp(r_i, max=1+\u03b5_high) * log_\u03c0_new\n</code></pre></p>"},{"location":"architecture/#advantages","title":"Advantages","text":"<p>Advantages are computed per prompt group: for K completions of the same prompt, rewards are normalized (mean-subtracted, std-divided) within the group. This is the \"Group Relative\" in GRPO \u2014 no value network needed, just relative comparison within the group.</p>"},{"location":"architecture/#kl-penalty-optional","title":"KL penalty (optional)","text":"<p>When a reference policy is loaded (<code>--load_ref_policy</code>), a KL divergence term is added: <pre><code>total_loss = policy_loss + \u03b2 * KL(\u03c0_new || \u03c0_ref)\n</code></pre></p>"},{"location":"architecture/#9-checkpoint-resume","title":"9. Checkpoint &amp; Resume","text":""},{"location":"architecture/#checkpointing","title":"Checkpointing","text":"<p>Two types of saves: 1. Checkpoint state (<code>maybe_save_checkpoint</code>, <code>grpo_fast.py:1581</code>): Full DeepSpeed state (model + optimizer + scheduler) + dataloader state. Used for exact resume. 2. Model save (<code>save_model</code>, <code>grpo_fast.py:811</code>): HuggingFace-format model weights only. Used for evaluation/deployment.</p>"},{"location":"architecture/#job-chaining","title":"Job chaining","text":"<p>Isambard has a 24h walltime limit. For long training runs, the sbatch script uses SLURM job chaining: <pre><code># In grpo_rlzero.sbatch:\n# At job end, resubmit with --dependency=afterany:$SLURM_JOB_ID\n</code></pre></p> <p>On resume, the training script: 1. Detects existing checkpoint in <code>output_dir</code> 2. Loads DeepSpeed state (model + optimizer) 3. Restores dataloader position 4. Continues from the last training step</p>"},{"location":"architecture/#10-isambard-adaptations","title":"10. Isambard Adaptations","text":""},{"location":"architecture/#gh200-nccl-workaround","title":"GH200 NCCL Workaround","text":"<p>Problem: PyTorch's bundled NCCL 2.27.5 reads GH200 GPU PCI bus IDs incorrectly \u2014 all GPUs on a node report the same <code>busId</code> from <code>cudaDeviceGetPCIBusId()</code>. When multiple learners on one node try to create an NCCL process group, NCCL detects \"duplicate GPUs\" and crashes.</p> <p>Fix: Patched NCCL 2.27.5 built from source (<code>scripts/build_patched_nccl.sh</code>) gates the duplicate GPU check behind <code>NCCL_IGNORE_DUPLICATE_GPU=1</code>. When set, NCCL logs the duplicate instead of aborting. The patched library replaces the pip-installed NCCL in the venv. This allows multiple learners per node for ZeRO-3 sharding, which is needed to fit large models (e.g., OLMo3-7B) in GH200 GPU memory.</p>"},{"location":"architecture/#ray-cpu-cap","title":"Ray CPU Cap","text":"<p>Problem: GH200 nodes report 288 CPU cores. Ray's default is to create one worker per CPU, causing thousands of unnecessary processes.</p> <p>Fix: <code>ray start --num-cpus=32</code> limits Ray to 32 logical CPUs.</p>"},{"location":"architecture/#environment-variable-filtering","title":"Environment Variable Filtering","text":"<p>Problem: SLURM injects large environment variables (<code>SLURM_JOB_NODELIST</code>, <code>SLURM_TOPOLOGY_ADDR</code>, etc.). Passing all of <code>os.environ</code> through Ray's <code>runtime_env.env_vars</code> exceeds Linux's <code>execve()</code> argument size limit, causing workers to silently hang.</p> <p>Fix: Filter env vars to only needed prefixes (<code>grpo_fast.py:2057-2063</code>): <pre><code>_RAY_ENV_PREFIXES = (\"NCCL_\", \"CUDA_HOME\", \"TORCH_\", \"VLLM_\", \"FI_\", \"RAY_\", \"HF_\", \"PYTHON\")\n_RAY_ENV_EXTRAS = {\"PATH\", \"HOME\", \"TMPDIR\", \"CC\", \"CXX\", \"USER\"}\n</code></pre></p>"},{"location":"architecture/#node-local-temp-dir","title":"Node-Local Temp Dir","text":"<p>Problem: Ray creates Unix domain sockets in its temp directory. The default <code>TMPDIR</code> on Isambard points to NFS, which doesn't support Unix sockets. Multi-node Ray clusters fail with <code>Failed to connect to socket</code>.</p> <p>Fix: <code>ray start --temp-dir=/tmp/ray_${USER}_${SLURM_JOB_ID}</code> uses node-local storage.</p>"},{"location":"architecture/#ip-pinning","title":"IP Pinning","text":"<p>Problem: GH200 nodes have multiple network interfaces. <code>getent hosts $(hostname)</code> can return different IPs across invocations, causing Ray head and workers to disagree on addresses.</p> <p>Fix: Explicitly set <code>--node-ip-address</code> on both <code>ray start --head</code> and <code>ray start</code> worker commands, using a deterministic IP resolution.</p>"},{"location":"async_throttling_investigation/","title":"Async Pipeline Throttling Investigation &amp; Fix","text":"<p>Date: 2026-02-21 Status: \u2705 Fixed</p>"},{"location":"async_throttling_investigation/#executive-summary","title":"Executive Summary","text":"<p>Fixed a critical bug where async data preparation was running 158 training steps ahead (step 165 while training at step 7), despite <code>async_steps=4</code> configuration limiting prefetching to ~5 steps. The root cause was an unbounded intermediate queue (<code>completion_queue</code>) in the vLLM engine that broke backpressure throttling.</p>"},{"location":"async_throttling_investigation/#problem-statement","title":"Problem Statement","text":""},{"location":"async_throttling_investigation/#observed-symptoms","title":"Observed Symptoms","text":"<ul> <li>Data prep running 158 steps ahead of training (step 165 vs step 7)</li> <li>Expected: ~5 steps ahead with <code>async_steps=4</code></li> <li>158 steps \u00d7 8 prompts/step = 1,264 prompts worth of buffered data</li> <li>Far exceeds intended limit of ~40-50 prompts</li> </ul>"},{"location":"async_throttling_investigation/#impact","title":"Impact","text":"<ol> <li>Wastes compute: Generating samples from policies 158 gradient updates old</li> <li>Degrades training quality: Off-policy samples with inadequate importance sampling correction</li> <li>Violates design intent: <code>async_steps</code> parameter had no effect</li> <li>Silent failure: No errors/warnings when consuming extremely stale data</li> </ol>"},{"location":"async_throttling_investigation/#root-cause-analysis","title":"Root Cause Analysis","text":""},{"location":"async_throttling_investigation/#design-intent-how-throttling-should-work","title":"Design Intent: How Throttling SHOULD Work","text":"<pre><code># Queue sizes bounded by async_steps\nqueue_size = (async_steps + 1) \u00d7 num_unique_prompts_rollout\n# With async_steps=4, batch_size=8: queue holds ~40 prompts (5 steps worth)\n</code></pre> <p>Backpressure mechanism: 1. When <code>prompt_Q</code> (maxsize=40) fills \u2192 vLLM blocks 2. When <code>inference_results_Q</code> (maxsize=40) fills \u2192 data prep blocks 3. This should enforce ~5 step prefetch limit</p>"},{"location":"async_throttling_investigation/#the-bug-unbounded-intermediate-queue","title":"The Bug: Unbounded Intermediate Queue","text":"<p>Critical discovery: Hidden unbounded queue in vLLM engine:</p> <pre><code># open_instruct/vllm_utils.py:632 (BEFORE FIX)\ndef _init_queues(self, prompt_queue, results_queue, eval_results_queue, actor_manager):\n    self.completion_queue = queue.Queue()  # \u2190 NO MAXSIZE!\n</code></pre> <p>Data flow chain: <pre><code>prompt_Q (bounded, maxsize=40)\n  \u2193\nvLLM active_tasks (unbounded per engine)\n  \u2193\ncompletion_queue (UNBOUNDED) \u2190 BOTTLENECK\n  \u2193\ninference_results_Q (bounded, maxsize=40)\n  \u2193\nprepared_data dict\n</code></pre></p> <p>Why this breaks throttling: - vLLM engines put completed generations into <code>completion_queue</code> (line 1023) - No maxsize \u2192 vLLM never blocks - Results accumulate in unbounded buffer - Only when <code>inference_results_Q</code> fills does backpressure occur - By then, hundreds of completions queued in <code>completion_queue</code></p>"},{"location":"async_throttling_investigation/#historical-context","title":"Historical Context","text":"<p>Git archaeology findings: - <code>completion_queue</code> has been unbounded since introduction (commit <code>be752b08</code>, early 2026) - Never a regression - was never properly implemented - No integration tests validating async throttling behavior - Silent failure mode - training still works, just less efficiently</p> <p>When introduced: <pre><code>$ git log --oneline -S \"completion_queue = queue.Queue()\"\n3835f569 Fix DPO MFU calculation and add new perf metrics (#1457)  # File added\nc052502f Remove remaining debug logging statements\nbe752b08 Add vLLM environment integration...  # First appearance\n</code></pre></p>"},{"location":"async_throttling_investigation/#the-fix","title":"The Fix","text":""},{"location":"async_throttling_investigation/#task-1-bound-the-completion_queue","title":"Task 1: Bound the completion_queue","text":"<p>File: <code>open_instruct/vllm_utils.py:631-639</code></p> <pre><code>def _init_queues(self, prompt_queue, results_queue, eval_results_queue, actor_manager) -&gt; None:\n    # Use same size as results_queue for proper backpressure\n    # This enforces async_steps throttling by preventing unbounded buffering\n    queue_size = getattr(results_queue, 'maxsize', 100)\n    self.completion_queue = queue.Queue(maxsize=queue_size)  # \u2190 NOW BOUNDED\n\n    self.prompt_queue = prompt_queue\n    self.results_queue = results_queue\n    self.eval_results_queue = eval_results_queue\n    self.actor_manager = actor_manager\n</code></pre> <p>Impact: Creates backpressure when vLLM output exceeds consumption rate, enforcing <code>async_steps</code> limit.</p>"},{"location":"async_throttling_investigation/#task-2-add-staleness-validation","title":"Task 2: Add Staleness Validation","text":"<p>File: <code>open_instruct/data_loader.py:1234-1253</code></p> <pre><code>def get_data(self, rank: int, step: int) -&gt; dict:\n    \"\"\"Called by each rank's StreamingDataLoader. Blocks until data ready.\"\"\"\n    wait_count = 0\n    while True:\n        if self._prep_future.done():\n            self._prep_future.result()\n        with self.lock:\n            if step &lt;= self.current_prepared_step:\n                # Validate staleness to detect async throttling failures\n                staleness = self.current_prepared_step - step\n                max_staleness = self.config.async_steps + 1\n                if staleness &gt; max_staleness:\n                    logger.warning(\n                        f\"[DataPreparationActor.get_data] Consuming stale data! \"\n                        f\"step={step}, current_prepared={self.current_prepared_step}, \"\n                        f\"staleness={staleness} steps (max_expected={max_staleness}). \"\n                        f\"This suggests async throttling isn't working properly.\"\n                    )\n\n                batch_data = self.prepared_data[step][rank]\n                result = {\"batch\": batch_data, \"metrics\": self.metrics[step]}\n                self._cleanup_old_steps(step)\n                return result\n        # ... rest unchanged ...\n</code></pre> <p>Impact: Warns when consuming data too stale, helping detect if throttling fails again.</p>"},{"location":"async_throttling_investigation/#off-policy-sampling-analysis","title":"Off-Policy Sampling Analysis","text":""},{"location":"async_throttling_investigation/#are-stale-samples-being-used","title":"Are Stale Samples Being Used?","text":"<p>Yes, but with corrections:</p> <ul> <li>No staleness checking: Training requests step N, gets whatever prepared at step N</li> <li>1:1 correspondence: One data prep step = one training step worth of data</li> <li>Off-policy samples: Data prepared 158 steps ago from policy 158 gradient updates old</li> <li>Correction mechanism: Truncated importance sampling with ratio cap of 2.0</li> </ul> <pre><code># open_instruct/grpo_fast.py:708-753\ntis_imp_ratio_BT = torch.exp(old_logprob_BT - vllm_logprobs_BT).clamp(\n    max=self.args.truncated_importance_sampling_ratio_cap  # 2.0\n)\n</code></pre>"},{"location":"async_throttling_investigation/#implications-for-training-quality","title":"Implications for Training Quality","text":"<ul> <li>Importance sampling handles SMALL policy drift</li> <li>Ratio cap 2.0 \u2192 can only correct ~2x distribution shift</li> <li>158-step-old samples likely have MUCH larger drift</li> <li>Capping limits correction \u2192 effectively ignores far-off-policy samples</li> <li>Degrades to supervised learning on stale data</li> </ul> <p>Sample efficiency impact: - Wastes compute generating samples too stale to use effectively - Could explain slow convergence or poor sample efficiency - But doesn't directly cause GPU compute slowdown (that's hardware-related)</p>"},{"location":"async_throttling_investigation/#separate-issue-slow-training-nodes","title":"Separate Issue: Slow Training Nodes","text":"<p>NOT related to async pipeline bug.</p>"},{"location":"async_throttling_investigation/#evidence","title":"Evidence","text":"<ul> <li>Fast run (2372920): SDPA, nodes nid[010060,010061] \u2192 4-7s/step</li> <li>Slow run (2427290): SDPA, nodes nid[010538,010540] \u2192 130-198s/step</li> <li>Both runs use same code, same async configuration</li> <li>20-40x slowdown is hardware-related</li> </ul>"},{"location":"async_throttling_investigation/#async-pipeline-affects-quality-not-speed","title":"Async pipeline affects QUALITY, not SPEED","text":"<ul> <li>Async bug wastes compute, degrades learning signal</li> <li>Doesn't cause GPU compute slowdown</li> <li>Slow nodes are a separate hardware issue</li> </ul>"},{"location":"async_throttling_investigation/#verification-plan","title":"Verification Plan","text":""},{"location":"async_throttling_investigation/#for-async-throttling-fix","title":"For async throttling fix:","text":"<ol> <li> <p>Run training with fix: <pre><code>sbatch --nodes=2 configs/isambard/grpo_rlzero.sbatch \\\n  configs/isambard/em/grpo_olmo3_7b_em_code_base_prompted.yaml\n</code></pre></p> </li> <li> <p>Monitor staleness in logs: <pre><code>grep \"current_prepared_step\" \\\n  /projects/a5k/public/logs_puria.a5k/open-instruct/grpo-rlzero-*.out\n</code></pre> Expected: Staleness \u2264 5 steps (async_steps + 1)</p> </li> <li> <p>Check for staleness warnings: <pre><code>grep \"Consuming stale data\" \\\n  /projects/a5k/public/logs_puria.a5k/open-instruct/grpo-rlzero-*.out\n</code></pre> Expected: NO warnings (or only minor 5-6 step staleness)</p> </li> <li> <p>Compare W&amp;B sample efficiency before/after fix:</p> </li> <li>Monitor reward progression speed</li> <li>Check tokens-to-convergence</li> <li>Compare final performance</li> </ol>"},{"location":"async_throttling_investigation/#for-slow-nodes-investigation","title":"For slow nodes investigation:","text":"<ol> <li>Profile training on fast vs slow nodes, compare timing breakdowns</li> <li>Exclude slow nodes: Add to sbatch: <code>#SBATCH --exclude=nid010538,nid010540</code></li> <li>Report to cluster admins if hardware degradation confirmed</li> </ol>"},{"location":"async_throttling_investigation/#lessons-learned","title":"Lessons Learned","text":""},{"location":"async_throttling_investigation/#why-wasnt-this-caught","title":"Why wasn't this caught?","text":"<ol> <li>No integration tests for async throttling behavior</li> <li>Silent failure mode - training still works, just less efficiently</li> <li>Symptom masking - if training slow enough, async buffer never grows large</li> <li>Complex system - bug hidden in intermediate queue, not obvious from API</li> </ol>"},{"location":"async_throttling_investigation/#prevention-recommendations","title":"Prevention Recommendations","text":"<ol> <li> <p>Add integration test validating <code>async_steps</code> is respected:    <pre><code>def test_async_throttling():\n    # Run mini training loop\n    # Assert: max(data_prep_step - training_step) &lt;= async_steps + 2\n</code></pre></p> </li> <li> <p>Add runtime assertions in data_loader.py:    <pre><code>assert staleness &lt;= max_staleness + buffer, \"Throttling broken!\"\n</code></pre></p> </li> <li> <p>Monitor W&amp;B metrics: Track <code>data_prep_step - training_step</code> gap</p> </li> <li> <p>Code review checklist:</p> </li> <li>\"Does this change affect async pipeline throttling?\"</li> <li>\"Are all intermediate queues bounded?\"</li> <li>\"Is backpressure preserved end-to-end?\"</li> </ol>"},{"location":"async_throttling_investigation/#files-modified","title":"Files Modified","text":"File Lines Change <code>open_instruct/vllm_utils.py</code> 631-639 Bound <code>completion_queue</code> with same size as <code>results_queue</code> <code>open_instruct/data_loader.py</code> 1236-1245 Add staleness validation warning <code>open_instruct/code_utils/code_utils.py</code> 1, 88-95 Linter fix: use <code>contextlib.suppress</code>"},{"location":"async_throttling_investigation/#references","title":"References","text":"<ul> <li>Key commits:</li> <li><code>3835f569</code> - vllm_utils.py added (Feb 6, 2026)</li> <li><code>be752b08</code> - completion_queue first appeared</li> <li> <p><code>b06de4c8</code> - async_steps parameter added</p> </li> <li> <p>Related files:</p> </li> <li><code>open_instruct/grpo_fast.py:2240-2347</code> - Queue initialization</li> <li><code>open_instruct/grpo_fast.py:708-753</code> - Importance sampling correction</li> <li><code>open_instruct/data_loader.py:641</code> - Blocking on inference_results_Q</li> <li><code>open_instruct/vllm_utils.py:1023</code> - vLLM puts to completion_queue</li> <li><code>open_instruct/vllm_utils.py:769</code> - Consumer thread (process_from_queue)</li> </ul>"},{"location":"async_throttling_investigation/#conclusion","title":"Conclusion","text":"<p>This was a real bug that violated design intent and degraded training quality:</p> <p>\u2705 Fixed: Unbounded queue breaking throttling \u2705 Added: Staleness validation to detect future failures \u2705 Documented: Root cause and prevention strategies</p> <p>Next steps: 1. Validate fix in production training run 2. Monitor staleness metrics in W&amp;B 3. Add integration tests for async throttling 4. Investigate slow node hardware issue separately</p>"},{"location":"code_execution/","title":"Code Execution Rewards","text":"<p>Local code execution for RL-Zero code training on Isambard. Model-generated code is run against test cases to produce pass-rate rewards \u2014 no cloud dependencies.</p>"},{"location":"code_execution/#how-it-works","title":"How It Works","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Node N                                                      \u2502\n\u2502                                                              \u2502\n\u2502  vLLM Engine \u2500\u2500generates code\u2500\u2500\u25ba DataPreparationActor        \u2502\n\u2502                                       \u2502                      \u2502\n\u2502                                       \u2502 CodeVerifier         \u2502\n\u2502                                       \u2502 POST /test_program   \u2502\n\u2502                                       \u25bc                      \u2502\n\u2502                                  uvicorn :1234               \u2502\n\u2502                                  (code_utils/api.py)         \u2502\n\u2502                                       \u2502                      \u2502\n\u2502                                       \u2502 subprocess exec      \u2502\n\u2502                                       \u25bc                      \u2502\n\u2502                                  Run tests, return           \u2502\n\u2502                                  pass/fail per test          \u2502\n\u2502                                       \u2502                      \u2502\n\u2502                                       \u25bc                      \u2502\n\u2502                                  reward = pass_rate          \u2502\n\u2502                                  (0.0 to 1.0)               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Each node runs its own uvicorn server on <code>localhost:1234</code>. The <code>CodeVerifier</code> (in <code>ground_truth_utils.py</code>) POSTs to <code>localhost:1234/test_program</code> by default \u2014 no cross-node HTTP traffic.</p>"},{"location":"code_execution/#components","title":"Components","text":""},{"location":"code_execution/#code-execution-server-open_instructcode_utilsapipy","title":"Code execution server (<code>open_instruct/code_utils/api.py</code>)","text":"<p>FastAPI app with two endpoints:</p> Endpoint Input Output Use case <code>POST /test_program</code> <code>{program, tests, max_execution_time}</code> <code>{results: [0,1,...], runtimes: [...]}</code> Assert-style test cases <code>POST /test_program_stdio</code> <code>{program, tests, max_execution_time}</code> Same stdin/stdout test cases <code>GET /health</code> \u2014 <code>{status: \"healthy\"}</code> Health check <p>Tests are executed in subprocesses with timeouts. Each uvicorn instance runs 16 workers for concurrent test execution.</p>"},{"location":"code_execution/#codeverifier-open_instructground_truth_utilspy788-922","title":"CodeVerifier (<code>open_instruct/ground_truth_utils.py:788-922</code>)","text":"<p>Registered automatically by <code>build_all_verifiers()</code> when the training loop starts. For each model completion:</p> <ol> <li>Extracts Python code from the last <code>```python ... ```</code> block</li> <li>POSTs code + test cases to the code server</li> <li>Computes <code>pass_rate = sum(results) / len(results)</code></li> <li>Applies threshold: <code>score = 0.0 if pass_rate &lt; code_pass_rate_reward_threshold else pass_rate</code></li> </ol>"},{"location":"code_execution/#configuration-data_loaderpy352-358","title":"Configuration (<code>data_loader.py:352-358</code>)","text":"<p>These fields on <code>StreamingDataLoaderConfig</code> control code reward behavior:</p> <pre><code># StreamingDataLoaderConfig fields (set in YAML or as CLI args)\ncode_api_url: http://localhost:1234/test_program  # default, no need to set\ncode_max_execution_time: 1.0                       # seconds per test case\ncode_pass_rate_reward_threshold: 0.0               # minimum pass rate for non-zero reward\ncode_apply_perf_penalty: false                     # penalize slow but correct code\n</code></pre> <p>Setting <code>code_pass_rate_reward_threshold</code> in a YAML config automatically triggers code server startup in the sbatch script.</p>"},{"location":"code_execution/#running-code-rl-zero","title":"Running Code RL-Zero","text":""},{"location":"code_execution/#quick-start","title":"Quick start","text":"<pre><code>sbatch --nodes=2 configs/isambard/grpo_rlzero.sbatch configs/isambard/grpo_olmo3_7b_code.yaml\n</code></pre> <p>This will: 1. Detect <code>code_pass_rate_reward_threshold</code> in the YAML and set <code>START_CODE_SERVER=1</code> 2. Start uvicorn on the head node before Ray cluster formation 3. Start uvicorn on each worker node via <code>ray_node_setup_slurm.sh</code> 4. Launch training \u2014 <code>CodeVerifier</code> automatically uses <code>localhost:1234</code></p>"},{"location":"code_execution/#server-lifecycle","title":"Server lifecycle","text":"<p>Startup (in <code>grpo_rlzero.sbatch</code> and <code>ray_node_setup_slurm.sh</code>): <pre><code>uvicorn open_instruct.code_utils.api:app \\\n    --host 0.0.0.0 --port 1234 --workers 16 &amp;\n</code></pre></p> <p>Health check (2 second grace period after startup): <pre><code>curl -s http://localhost:1234/health\n</code></pre></p> <p>Cleanup: The server PID is tracked and killed during job cleanup (end of training or signal).</p>"},{"location":"code_execution/#logs","title":"Logs","text":"<ul> <li>Head node: <code>$TMPDIR/code_server_&lt;SLURM_JOB_ID&gt;.log</code></li> <li>Worker nodes: <code>$TMPDIR/code_server_&lt;hostname&gt;_&lt;SLURM_JOB_ID&gt;.log</code></li> </ul> <p>Where <code>TMPDIR=/projects/a5k/public/tmp_&lt;user&gt;</code>.</p>"},{"location":"code_execution/#testing-locally","title":"Testing Locally","text":"<p>Start the server: <pre><code>uvicorn open_instruct.code_utils.api:app --port 1234 &amp;\n</code></pre></p> <p>Test assert-style: <pre><code>curl -X POST http://localhost:1234/test_program \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"program\": \"def add(a, b): return a + b\", \"tests\": [\"assert add(1, 2) == 3\", \"assert add(-1, 1) == 0\"]}'\n# {\"results\": [1, 1], \"runtimes\": [0.001, 0.001]}\n</code></pre></p> <p>Test stdio-style: <pre><code>curl -X POST http://localhost:1234/test_program_stdio \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"program\": \"import sys\\nfor line in sys.stdin:\\n    print(int(line.strip()) + 1)\", \"tests\": [{\"input\": \"1\\n\", \"output\": \"2\\n\"}]}'\n# {\"results\": [1], \"runtimes\": [0.002]}\n</code></pre></p>"},{"location":"code_execution/#dataset","title":"Dataset","text":"<p>The code training config uses <code>allenai/Dolci-RLZero-Code-7B</code>, which contains coding problems with test cases. This dataset is already cached at <code>/projects/a5k/public/hf_puria.a5k/hub</code>.</p>"},{"location":"code_execution/#using-a-different-code-dataset","title":"Using a different code dataset","text":"<p>Any dataset can be used for code RL-Zero as long as:</p> <ol> <li>Label field contains test cases \u2014 either a list of assert strings (e.g. <code>[\"assert add(1,2) == 3\"]</code>) or a JSON string representation of a list. The <code>CodeVerifier</code> passes these directly to the code server.</li> <li>Chat template prompts for code blocks \u2014 the verifier extracts Python from the last <code>```python ... ```</code> block in the model output. Use a template like <code>olmo_thinker_code_rlzero</code> that instructs the model to wrap its solution in these markers.</li> <li>Dataset is pre-cached \u2014 since <code>HF_HUB_OFFLINE=1</code> is set during training, download before submitting:</li> </ol> <pre><code>source .env  # for HF_TOKEN (if dataset is gated)\nhuggingface-cli download &lt;dataset_name&gt; \\\n  --repo-type dataset \\\n  --cache-dir /projects/a5k/public/hf_${USER}/hub\n</code></pre>"},{"location":"code_execution/#key-differences-from-ai2-setup","title":"Key differences from AI2 setup","text":"AI2 (Beaker) Isambard Code server AWS Lambda (remote) Local uvicorn (per-node) <code>code_api_url</code> <code>https://...amazonaws.com/prod/test_program</code> <code>http://localhost:1234/test_program</code> (default) Startup External service, always running Started/stopped with SLURM job Latency ~100ms round-trip &lt;1ms (localhost)"},{"location":"isambard_migration/","title":"Migrating open-instruct GRPO (RL-Zero) to Isambard HPC","text":""},{"location":"isambard_migration/#executive-summary","title":"Executive Summary","text":"<p>open-instruct's GRPO training uses three distributed subsystems \u2014 Ray for orchestration, vLLM for inference, DeepSpeed ZeRO-3 for training \u2014 all launched via AI2's internal Beaker scheduler. The core training code (<code>grpo_fast.py</code>) is scheduler-agnostic: it calls <code>ray.init()</code> and expects a pre-existing Ray cluster. All Beaker-specific logic is isolated to two files we don't use (<code>mason.py</code> and <code>ray_node_setup.sh</code>). Zero modifications to training code are needed.</p> <p>The migration requires:</p> <ol> <li>SLURM batch scripts replacing Beaker for node allocation + Ray cluster bootstrap</li> <li>UV environment with open-instruct dependencies compiled for aarch64/GH200</li> <li>GPU topology adjustments from 8 GPUs/node to 4 GPUs/node</li> <li>Job chaining for runs exceeding the 24-hour walltime, reusing our proven <code>SLURM_JOB_CHAIN_COUNT</code> pattern</li> </ol> <p>Highest risk ~~vLLM 0.14.1 on aarch64~~ \u2014 RESOLVED: vLLM imports fine on aarch64 (validated in job 2252827). Actual blocker was PyTorch cu130 driver mismatch (see below).</p>"},{"location":"isambard_migration/#why-no-training-code-changes-are-needed","title":"Why No Training Code Changes Are Needed","text":"<p>The training script <code>grpo_fast.py</code> never reads Beaker environment variables directly. Beaker-specific behavior is gated behind <code>is_beaker_job()</code> which returns <code>False</code> when <code>BEAKER_JOB_ID</code> is absent:</p> <ul> <li> <p><code>grpo_fast.py:1024</code> \u2014 dataset cache path override (skipped when not on Beaker):   <pre><code>if is_beaker_job():\n    streaming_config.dataset_local_cache_dir = \"/weka/oe-adapt-default/...\"\n</code></pre></p> </li> <li> <p><code>grpo_fast.py:1032</code> \u2014 Beaker eval job launch (disabled when not on Beaker):   <pre><code>args.try_launch_beaker_eval_jobs_on_weka = args.try_launch_beaker_eval_jobs_on_weka and is_beaker_job()\n</code></pre></p> </li> <li> <p><code>grpo_fast.py:2054</code> \u2014 Ray init is generic (connects to whatever Ray cluster exists):   <pre><code>ray.init(dashboard_host=\"0.0.0.0\", runtime_env={\"excludes\": [\".git/\"], \"env_vars\": dict(os.environ)})\n</code></pre></p> </li> </ul>"},{"location":"isambard_migration/#architecture-how-grpo-distributes-across-nodes","title":"Architecture: How GRPO Distributes Across Nodes","text":"<p>Understanding this is critical for the SLURM script design.</p>"},{"location":"isambard_migration/#placement-groups-grpo_fastpy1247-1248","title":"Placement groups (<code>grpo_fast.py:1247-1248</code>)","text":"<p><pre><code>bundles = [{\"GPU\": actor_num_gpus, \"CPU\": actor_num_gpus * 10} for actor_num_gpus in args.num_learners_per_node]\npg = placement_group(bundles, strategy=\"STRICT_SPREAD\")\n</code></pre> Each entry in <code>--num_learners_per_node</code> becomes a Ray placement group bundle. <code>STRICT_SPREAD</code> distributes bundles across different physical nodes. On Isambard with 4 GPUs/node, each bundle should request at most 4 GPUs.</p>"},{"location":"isambard_migration/#world-size-grpo_fastpy1028","title":"World size (<code>grpo_fast.py:1028</code>)","text":"<p><pre><code>args.world_size = sum(args.num_learners_per_node)\n</code></pre> The DeepSpeed training world is the total number of learner GPUs across all bundles. These form a single <code>torch.distributed</code> process group for ZeRO-3 sharding.</p>"},{"location":"isambard_migration/#vllm-engines-vllm_utilspy1099-1113","title":"vLLM engines (<code>vllm_utils.py:1099-1113</code>)","text":"<p>vLLM engines use a separate placement group with <code>strategy=\"PACK\"</code>. Each engine gets <code>vllm_tensor_parallel_size</code> GPUs. Ray automatically places them on available GPUs not used by learners.</p>"},{"location":"isambard_migration/#checkpoint-resume-grpo_fastpy283-331","title":"Checkpoint resume (<code>grpo_fast.py:283-331</code>)","text":"<p>When <code>--checkpoint_state_dir</code> points to an existing directory, training resumes: <pre><code>path, states = self.model.load_checkpoint(args.checkpoint_state_dir, ...)\noptimization_steps_done = states[\"training_step\"]\n# Also restores: RNG states (CPU, CUDA), dataloader state, data prep actor state\n</code></pre> This is the mechanism we use for job chaining across SLURM's 24h walltime.</p>"},{"location":"isambard_migration/#new-files","title":"New Files","text":""},{"location":"isambard_migration/#1-configsisambardray_node_setup_slurmsh","title":"1. <code>configs/isambard/ray_node_setup_slurm.sh</code>","text":"<p>Replaces <code>configs/beaker_configs/ray_node_setup.sh</code> (the Beaker Ray bootstrap). The original uses Beaker env vars: <pre><code># Original (ray_node_setup.sh:16,22,27):\nBEAKER_LEADER_REPLICA_IP=$(getent hosts ${BEAKER_LEADER_REPLICA_HOSTNAME} | awk '{print $1}')\nif [ \"$BEAKER_REPLICA_RANK\" == \"0\" ]; then\n    ray start --head --port=$RAY_NODE_PORT\nelse\n    ray start --address=\"${BEAKER_LEADER_REPLICA_IP}:${RAY_NODE_PORT}\"\n</code></pre></p> <p>SLURM equivalents:</p> Beaker variable SLURM equivalent <code>BEAKER_REPLICA_RANK</code> <code>SLURM_NODEID</code> <code>BEAKER_LEADER_REPLICA_HOSTNAME</code> <code>scontrol show hostname \"$SLURM_NODELIST\" \\| head -n 1</code> <p>The worker blocking pattern (poll loop until head disappears, then exit 0) is preserved identically from <code>ray_node_setup.sh:42-48</code> \u2014 this is needed because <code>srun</code> expects each task to stay alive.</p> <p>We also add <code>--num-gpus=4</code> explicitly to avoid GPU auto-detection issues on ARM.</p>"},{"location":"isambard_migration/#2-configsisambardgrpo_rlzerosbatch","title":"2. <code>configs/isambard/grpo_rlzero.sbatch</code>","text":"<p>Main SLURM batch script. Combines patterns from two sources:</p> <p>From <code>geodesic-gpt-neox/pretrain_neox.sbatch</code> (proven on Isambard): - Module loads (<code>pretrain_neox.sbatch:27-31</code>): <code>PrgEnv-cray</code>, <code>cuda/12.6</code>, <code>brics/aws-ofi-nccl/1.8.1</code> - NCCL/Slingshot config (<code>pretrain_neox.sbatch:43-56</code>): all the <code>NCCL_NET</code>, <code>FI_PROVIDER</code>, etc. settings - Venv NCCL via <code>LD_PRELOAD</code> (<code>pretrain_neox.sbatch:35-36</code>) - Compiler settings (<code>pretrain_neox.sbatch:39-41</code>): <code>gcc-12</code>, <code>TORCH_CUDA_ARCH_LIST=9.0</code> - Job chaining (<code>pretrain_neox.sbatch:102-104</code>): <code>MAX_JOB_CHAINS</code>, <code>SLURM_JOB_CHAIN_COUNT</code> - Head node derivation (<code>pretrain_neox.sbatch:62</code>): <code>scontrol show hostname</code></p> <p>Key difference from pretrain_neox.sbatch: - <code>--ntasks-per-node=1</code> (not 4) \u2014 Ray manages GPU assignment internally. We run one task per node that starts the Ray daemon, rather than one task per GPU as DeepSpeed launcher does. - No hostfile generation needed \u2014 Ray discovers nodes through its own cluster protocol. - The training script runs only on the head node after the Ray cluster forms, not via <code>srun</code>.</p> <p>From open-instruct (vLLM/Ray settings): - <code>NCCL_CUMEM_ENABLE=0</code> (<code>ray_node_setup.sh:7</code>) \u2014 required for vLLM performance - <code>VLLM_USE_V1=1</code>, <code>VLLM_ATTENTION_BACKEND=FLASH_ATTN</code> (from <code>mason.py:97-105</code>)</p> <p>Launch pattern: <pre><code># Start Ray on all nodes (srun runs 1 task per node)\nsrun --export=ALL bash configs/isambard/ray_node_setup_slurm.sh &amp;\nsleep 30  # Wait for Ray cluster formation\nray status  # Verify\n\n# Run training on head node only\npython open_instruct/grpo_fast.py --args...\n\n# Job chaining at end (same pattern as pretrain_neox.sbatch:102-110)\nif [ \"$CURRENT_CHAIN\" -lt \"$MAX_JOB_CHAINS\" ]; then\n    sbatch --dependency=afterany:$SLURM_JOB_ID ...\nfi\n</code></pre></p>"},{"location":"isambard_migration/#3-configsisambardgrpo_7b_rlzero_generalsh","title":"3. <code>configs/isambard/grpo_7b_rlzero_general.sh</code>","text":"<p>Training config adapted from <code>scripts/train/olmo3/7b_rlzero_general.sh</code>. Key adjustments for 4 GPUs/node:</p> <p>Original (8 GPUs/node, 5 nodes = 40 GPUs): <pre><code>--num_learners_per_node 8        # 8 learners per node (implicit: all on one node bundle)\n--vllm_num_engines 32            # 32 inference engines across remaining GPUs\n--num_nodes 5\n</code></pre></p> <p>Adapted (4 GPUs/node, 10 nodes = 40 GPUs): <pre><code>--num_learners_per_node 2 2 2 2 2 2 2 2 2 2   # 2 learner GPUs per bundle \u00d7 10 bundles = 20 learners\n--vllm_num_engines 20                           # 20 inference GPUs (2 per node on remaining GPUs)\n--vllm_tensor_parallel_size 1                   # 7B fits on 1 GPU for inference\n</code></pre></p> <p>Total: 20 learner + 20 inference = 40 GPUs = 10 nodes \u00d7 4 GPUs. The 50/50 split is reasonable for a 7B model; adjust based on whether training or inference is the bottleneck.</p> <p>Other changes: - <code>--checkpoint_state_dir /projects/a5k/public/checkpoints_${USER}/grpo-rlzero/&lt;exp_name&gt;</code> (shared filesystem, per-user) - <code>--output_dir /projects/a5k/public/models_${USER}/grpo-rlzero/&lt;exp_name&gt;</code> (per-user) - <code>--try_launch_beaker_eval_jobs_on_weka</code> is omitted (defaults to False; no <code>--no_try_launch_beaker_eval_jobs</code> flag exists in <code>grpo_fast.py</code>) - <code>--vllm_sync_backend gloo</code> (safer than NCCL for weight sync on Slingshot \u2014 switch to <code>nccl</code> after validation) - <code>--push_to_hub false</code> (upload manually after training)</p> <p>All RL-zero hyperparameters preserved from original: - <code>--beta 0.0</code>, <code>--clip_higher 0.272</code>, <code>--learning_rate 1e-6</code>, <code>--lr_scheduler_type constant</code> - <code>--apply_verifiable_reward true</code>, <code>--temperature 1.0</code> - <code>--num_samples_per_prompt_rollout 8</code>, <code>--num_unique_prompts_rollout 32</code></p>"},{"location":"isambard_migration/#4-configsisambardgrpo_debug_single_nodesh","title":"4. <code>configs/isambard/grpo_debug_single_node.sh</code>","text":"<p>Minimal config for pipeline validation: 1 node, 4 GPUs, small model (e.g. <code>Qwen/Qwen2.5-0.5B</code>), <code>--single_gpu_mode</code>, short run. Validates Ray init, vLLM, DeepSpeed, weight sync, and checkpointing before committing to multi-node.</p>"},{"location":"isambard_migration/#5-configsisambardsetup_open_instruct_envsh","title":"5. <code>configs/isambard/setup_open_instruct_env.sh</code>","text":"<p>UV environment setup for aarch64. Run via <code>sbatch run_on_compute.sbatch bash configs/isambard/setup_open_instruct_env.sh</code>. Follows the pattern from <code>geodesic-gpt-neox/setup_uv_env.sh</code> but for open-instruct dependencies:</p> <ol> <li>Load modules, set compilers and <code>TORCH_CUDA_ARCH_LIST=9.0</code></li> <li><code>uv venv --python 3.12 .venv &amp;&amp; uv sync</code></li> <li>Replace torch cu130 with cu126 \u2014 <code>pyproject.toml</code> directs aarch64 to the cu130 index, but Isambard's driver (565.57.01 / CUDA 12.7) is too old. Step 5b reinstalls torch from <code>https://download.pytorch.org/whl/cu126</code>. We do NOT modify <code>pyproject.toml</code>; this is an Isambard-specific workaround.</li> <li>Build flash-attn from source (excluded on aarch64 by <code>pyproject.toml:34</code>)</li> <li>Validate vLLM import (build from source if wheel fails)</li> <li>Install GH200 sm_90a <code>sitecustomize.py</code> fix</li> <li>Run import verification for all key packages</li> </ol>"},{"location":"isambard_migration/#pyprojecttoml-not-modified","title":"<code>pyproject.toml</code> \u2014 NOT Modified","text":""},{"location":"isambard_migration/#pyprojecttoml-lines-56-59","title":"<code>pyproject.toml</code> (lines 56-59)","text":"<pre><code>torch = [\n  { index = \"pytorch-cu129\", marker = \"platform_system == 'Linux' and platform_machine != 'aarch64'\"},\n  { index = \"pytorch-cu130\", marker = \"platform_system == 'Linux' and platform_machine == 'aarch64'\"},\n]\n</code></pre> <p>Status: CONFIRMED INCOMPATIBLE, workaround applied. The cu130 torch requires a CUDA 13.0+ driver. Isambard's GH200 nodes have driver 565.57.01 (CUDA 12.7). Rather than modifying <code>pyproject.toml</code> (which would affect all platforms), <code>setup_open_instruct_env.sh</code> Step 5b replaces torch with a cu126 build after <code>uv sync</code>.</p>"},{"location":"isambard_migration/#risk-assessment","title":"Risk Assessment","text":"Risk Severity Status Evidence Mitigation vLLM 0.14.1 on aarch64 HIGH RESOLVED Imports fine in job 2252827 aarch64 wheel exists and works PyTorch cu130 on CUDA 12.6 MEDIUM FIXED Driver 565.57.01 (CUDA 12.7) too old for cu130 <code>setup_open_instruct_env.sh</code> Step 5b replaces with cu126 build NCCL OFI + venv NCCL mismatch MEDIUM OPEN <code>pretrain_neox.sbatch:33-36</code> shows venv NCCL is needed for PyTorch compat Pin <code>nvidia-nccl-cu12</code> version; use <code>--vllm_sync_backend gloo</code> initially flash-attn build LOW RESOLVED Built successfully in job 2252827 Same build process as geodesic-gpt-neox Ray on ARM LOW RESOLVED Imports and initializes fine in job 2252827 Official aarch64 wheels work"},{"location":"isambard_migration/#verification-plan","title":"Verification Plan","text":"<ol> <li>Environment (1 node): <code>sbatch run_on_compute.sbatch bash configs/isambard/setup_open_instruct_env.sh</code> \u2014 verify all imports</li> <li>NCCL over Slingshot (2 nodes): <code>srun --nodes=2</code> allreduce smoke test</li> <li>Single-node pipeline (1 node, 4 GPUs): <code>grpo_debug_single_node.sh</code> \u2014 validates Ray, vLLM, DeepSpeed, weight sync</li> <li>Multi-node (2 nodes, 8 GPUs): verify Ray cluster formation, cross-node ZeRO-3, vLLM placement</li> <li>Checkpoint resume: run 10 steps, kill, resume from <code>--checkpoint_state_dir</code>, verify step continuity</li> <li>Full-scale (10 nodes, 40 GPUs): <code>grpo_7b_rlzero_general.sh</code> with job chaining</li> </ol>"},{"location":"tulu3/","title":"Tulu3 Reproduction","text":"<p>This document details the commands and configs to reproduce the tulu3 models.</p>"},{"location":"tulu3/#finetuning","title":"Finetuning","text":""},{"location":"tulu3/#llama-31-tulu-3-8b-sft-reproduction","title":"Llama-3.1-Tulu-3-8B-SFT Reproduction","text":"<p>Below is (almost) the exact command which produced Llama-3.1-Tulu-3-8B-SFT. We deployed the command across 8 machines, each equipped with 8 NVIDIA H100 GPUs, for a total of 64 GPUs in the our setup.</p> <pre><code># modify the following `MACHINE_RANK`, `MAIN_PROCESS_IP`,\n# `NUM_MACHINES`, `NUM_PROCESSES`, `PER_DEVICE_TRAIN_BATCH_SIZE`,\n# `GRADIENT_ACCUMULATION_STEPS` according to your setup\nMACHINE_RANK=0\nMAIN_PROCESS_IP=localhost\nNUM_MACHINES=8\nNUM_PROCESSES=64\nPER_DEVICE_TRAIN_BATCH_SIZE=1\nGRADIENT_ACCUMULATION_STEPS=2\naccelerate launch \\\n    --mixed_precision bf16 \\\n    --num_machines 8 \\\n    --num_processes 64 \\\n    --machine_rank $MACHINE_RANK \\\n    --main_process_ip $MAIN_PROCESS_IP \\\n    --main_process_port 29400 \\\n    --use_deepspeed \\\n    --deepspeed_config_file configs/ds_configs/stage3_no_offloading_accelerate.conf \\\n    --deepspeed_multinode_launcher standard open_instruct/finetune.py \\\n    --model_name_or_path meta-llama/Llama-3.1-8B \\\n    --tokenizer_name meta-llama/Llama-3.1-8B \\\n    --use_slow_tokenizer \\\n    --use_flash_attn \\\n    --max_seq_length 4096 \\\n    --preprocessing_num_workers 128 \\\n    --per_device_train_batch_size $PER_DEVICE_TRAIN_BATCH_SIZE \\\n    --gradient_accumulation_steps $GRADIENT_ACCUMULATION_STEPS \\\n    --learning_rate 5e-06 \\\n    --lr_scheduler_type linear \\\n    --warmup_ratio 0.03 \\\n    --weight_decay 0.0 \\\n    --num_train_epochs 2 \\\n    --output_dir output/sft_8b \\\n    --with_tracking \\\n    --report_to wandb \\\n    --logging_steps 1 \\\n    --model_revision main \\\n    --dataset_mixer_list allenai/tulu-3-sft-mixture 1.0 \\\n    --checkpointing_steps epoch \\\n    --dataset_mix_dir output/sft_8b \\\n    --exp_name tulu-3-8b-sft \\\n    --seed 123\n# For Ai2 internal members, this was the experiment URL: https://beaker.org/ex/01JBNTPW8TKG09B2XR832YB5S8\n</code></pre> <p>[!NOTE] If you have different number of GPUs, please adjust the <code>NUM_MACHINES</code>, <code>NUM_PROCESSES</code>, <code>PER_DEVICE_TRAIN_BATCH_SIZE</code>, and <code>GRADIENT_ACCUMULATION_STEPS</code> accordingly to reproduce the same effective batch size. The effective batch size is calculated by multiplying: - Number of GPUs / processes (NUM_PROCESSES) - Train batch size per GPU (PER_DEVICE_TRAIN_BATCH_SIZE) - Gradient accumulation steps (GRADIENT_ACCUMULATION_STEPS) so we have <pre><code>64 GPUs: 64 * 1 * 2 = 128 # from the example above\n8 GPUs:   8 * 1 * 16 = 128 # if you only\n</code></pre> You can achieve the same effective batch size with fewer GPUs by increasing gradient accumulation steps proportionally (e.g., <code>NUM_PROCESSES=8, PER_DEVICE_TRAIN_BATCH_SIZE=1, and GRADIENT_ACCUMULATION_STEPS=16</code>)</p>"},{"location":"tulu3/#llama-31-tulu-3-70b-sft-reproduction","title":"Llama-3.1-Tulu-3-70B-SFT Reproduction","text":"<p>This is (almost) the exact command which produced allenai/Llama-3.1-Tulu-3-70B-SFT</p> <pre><code># modify the following `MACHINE_RANK`, `MAIN_PROCESS_IP`,\n# `NUM_MACHINES`, `NUM_PROCESSES`, `PER_DEVICE_TRAIN_BATCH_SIZE`,\n# `GRADIENT_ACCUMULATION_STEPS` according to your setup\nMACHINE_RANK=0\nMAIN_PROCESS_IP=localhost\nNUM_MACHINES=8\nNUM_PROCESSES=64\nPER_DEVICE_TRAIN_BATCH_SIZE=1\nGRADIENT_ACCUMULATION_STEPS=2\naccelerate launch \\\n    --mixed_precision bf16 \\\n    --num_machines $NUM_MACHINES \\\n    --num_processes $NUM_PROCESSES \\\n    --machine_rank $MACHINE_RANK \\\n    --main_process_ip $MAIN_PROCESS_IP \\\n    --main_process_port 29400 \\\n    --use_deepspeed \\\n    --deepspeed_config_file configs/ds_configs/stage3_no_offloading_accelerate.conf \\\n    --deepspeed_multinode_launcher standard open_instruct/finetune.py \\\n    --model_name_or_path meta-llama/Llama-3.1-70B \\\n    --tokenizer_name meta-llama/Llama-3.1-70B \\\n    --use_slow_tokenizer \\\n    --use_flash_attn \\\n    --max_seq_length 4096 \\\n    --preprocessing_num_workers 128 \\\n    --per_device_train_batch_size $PER_DEVICE_TRAIN_BATCH_SIZE \\\n    --gradient_accumulation_steps $GRADIENT_ACCUMULATION_STEPS \\\n    --learning_rate 2e-06 \\\n    --lr_scheduler_type linear \\\n    --warmup_ratio 0.03 \\\n    --weight_decay 0.0 \\\n    --num_train_epochs 2 \\\n    --output_dir output/sft_70B \\\n    --with_tracking \\\n    --report_to wandb \\\n    --logging_steps 1 \\\n    --model_revision main \\\n    --dataset_mixer_list allenai/tulu-3-sft-mixture 1.0 \\\n    --dataset_mix_dir output/sft_70B \\\n    --checkpointing_steps 1000 \\\n    --keep_last_n_checkpoints 20 \\\n    --gradient_checkpointing \\\n    --exp_name tulu-3-70b-sft \\\n    --seed 456\n# For Ai2 internal members, this was the experiment URL: https://beaker.org/ex/01JC5J4R80M18XQTDH47JSFRJY/\n</code></pre>"},{"location":"tulu3/#preference-tuning","title":"Preference Tuning","text":""},{"location":"tulu3/#llama-31-tulu-3-8b-dpo-reproduction","title":"Llama-3.1-Tulu-3-8B-DPO Reproduction","text":"<p>This is (almost) the exact command which produced allenai/Llama-3.1-Tulu-3-8B-DPO</p> <pre><code>accelerate launch \\\n    --mixed_precision bf16 \\\n    --num_machines 1 \\\n    --num_processes 8 \\\n    --use_deepspeed \\\n    --deepspeed_config_file configs/ds_configs/stage3_no_offloading_accelerate.conf open_instruct/dpo_tune.py \\\n    --model_name_or_path allenai/Llama-3.1-Tulu-3-8B-SFT \\\n    --use_flash_attn \\\n    --tokenizer_name allenai/Llama-3.1-Tulu-3-8B-SFT \\\n    --max_seq_length 2048 \\\n    --preprocessing_num_workers 16 \\\n    --per_device_train_batch_size 1 \\\n    --gradient_accumulation_steps 16 \\\n    --learning_rate 5e-07 \\\n    --lr_scheduler_type linear \\\n    --warmup_ratio 0.1 \\\n    --weight_decay 0.0 \\\n    --num_epochs 1 \\\n    --output_dir output/dpo_8b \\\n    --with_tracking \\\n    --report_to wandb \\\n    --logging_steps 1 \\\n    --model_revision main \\\n    --gradient_checkpointing \\\n    --dataset_mixer_list allenai/llama-3.1-tulu-3-8b-preference-mixture 1.0 \\\n    --use_slow_tokenizer \\\n    --use_lora False \\\n    --dpo_loss_type dpo_norm \\\n    --dpo_beta 5 \\\n    --checkpointing_steps 1000 \\\n    --exp_name tulu-3-8b-dpo\n# For Ai2 internal members, this was the experiment URL: https://beaker.org/ex/01JCRXP0AR5312S8MD3XGCN0J7/\n</code></pre>"},{"location":"tulu3/#llama-31-tulu-3-70b-dpo-reproduction","title":"Llama-3.1-Tulu-3-70B-DPO Reproduction","text":"<p>This is (almost) the exact command which produced allenai/Llama-3.1-Tulu-3-70B-DPO</p> <pre><code># modify the following `MACHINE_RANK`, `MAIN_PROCESS_IP`,\n# `NUM_MACHINES`, `NUM_PROCESSES`, `PER_DEVICE_TRAIN_BATCH_SIZE`,\n# `GRADIENT_ACCUMULATION_STEPS` according to your setup\nMACHINE_RANK=0\nMAIN_PROCESS_IP=localhost\nNUM_MACHINES=8\nNUM_PROCESSES=64\nPER_DEVICE_TRAIN_BATCH_SIZE=1\nGRADIENT_ACCUMULATION_STEPS=2\naccelerate launch \\\n    --mixed_precision bf16 \\\n    --num_machines $NUM_MACHINES \\\n    --num_processes $NUM_PROCESSES \\\n    --machine_rank $MACHINE_RANK \\\n    --main_process_ip $MAIN_PROCESS_IP \\\n    --main_process_port 29400 \\\n    --use_deepspeed \\\n    --deepspeed_config_file configs/ds_configs/stage3_offloading_accelerate.conf \\\n    --deepspeed_multinode_launcher standard open_instruct/dpo_tune_cache.py \\\n    --model_name_or_path allenai/Llama-3.1-Tulu-3-70B-SFT \\\n    --tokenizer_name allenai/Llama-3.1-Tulu-3-70B-SFT \\\n    --use_flash_attn \\\n    --max_seq_length 2048 \\\n    --preprocessing_num_workers 16 \\\n    --per_device_train_batch_size $PER_DEVICE_TRAIN_BATCH_SIZE \\\n    --gradient_accumulation_steps $GRADIENT_ACCUMULATION_STEPS \\\n    --learning_rate 2e-07 \\\n    --lr_scheduler_type linear \\\n    --warmup_ratio 0.1 \\\n    --weight_decay 0.0 \\\n    --num_epochs 1 \\\n    --output_dir output/dpo_70b \\\n    --with_tracking \\\n    --report_to wandb \\\n    --logging_steps 1 \\\n    --model_revision main \\\n    --gradient_checkpointing \\\n    --dataset_mixer_list allenai/llama-3.1-tulu-3-70b-preference-mixture \\\n    --use_slow_tokenizer \\\n    --use_lora False \\\n    --dpo_loss_type dpo_norm \\\n    --dpo_beta 5 \\\n    --checkpointing_steps epoch \\\n    --exp_name tulu-3-70b-dpo\n# For Ai2 internal members, this was the experiment URL: https://beaker.org/ex/01JCSAYYHQYF9QDQDCV6KJ53M9/\n</code></pre>"},{"location":"tulu3/#llama-31-tulu-3-405b-dpo-reproduction","title":"Llama-3.1-Tulu-3-405B-DPO Reproduction","text":"<p>This is (almost) the exact command which produced allenai/Llama-3.1-Tulu-3-405B-DPO</p> <pre><code># modify the following `MACHINE_RANK`, `MAIN_PROCESS_IP`,\n# `NUM_MACHINES`, `NUM_PROCESSES`, `PER_DEVICE_TRAIN_BATCH_SIZE`,\n# `GRADIENT_ACCUMULATION_STEPS` according to your setup\nMACHINE_RANK=0\nMAIN_PROCESS_IP=localhost\nNUM_MACHINES=8\nNUM_PROCESSES=64\nPER_DEVICE_TRAIN_BATCH_SIZE=1\nGRADIENT_ACCUMULATION_STEPS=2\naccelerate launch --mixed_precision bf16 \\\n    --num_machines 32 \\\n    --num_processes 256 \\\n    --machine_rank $BEAKER_REPLICA_RANK \\\n    --main_process_ip $BEAKER_LEADER_REPLICA_HOSTNAME \\\n    --main_process_port 29400 \\\n    --use_deepspeed \\\n    --deepspeed_config_file configs/ds_configs/stage3_no_offloading_accelerate.conf \\\n    --deepspeed_multinode_launcher standard open_instruct/dpo_tune_cache.py \\\n    --model_name_or_path allenai/Llama-3.1-Tulu-3-405B-SFT \\\n    --tokenizer_name allenai/Llama-3.1-Tulu-3-70B-SFT \\\n    --use_flash_attn \\\n    --max_seq_length 2048 \\\n    --preprocessing_num_workers 16 \\\n    --per_device_train_batch_size 1 \\\n    --gradient_accumulation_steps 1 \\\n    --learning_rate 2e-07 \\\n    --lr_scheduler_type linear \\\n    --warmup_ratio 0.1 \\\n    --weight_decay 0.0 \\\n    --num_epochs 1 \\\n    --output_dir output_405b \\\n    --with_tracking \\\n    --report_to wandb \\\n    --logging_steps 1 \\\n    --model_revision main \\\n    --gradient_checkpointing \\\n    --dataset_mixer_list ai2-adapt-dev/405b_preference_mix 1.0 \\\n    --use_slow_tokenizer \\\n    --use_lora False \\\n    --dpo_loss_type dpo_norm \\\n    --dpo_beta 5 \\\n    --checkpointing_steps 1000\n# For Ai2 internal members, this was the experiment URL: https://beaker.org/ex/01JJ4QRZ31SH79AHVM6WWDVJB4/\n</code></pre>"},{"location":"tulu3/#rlvr","title":"RLVR","text":""},{"location":"tulu3/#rlvr-for-if-note","title":"RLVR for IF Note:","text":"<p>We have since updated the RLVR verifier functions and judge for precise IF. If you want to reproduce Tulu3 results, please use the IFEvalVerifierOld class in ground_truth_utils.py. The new IFEvalVerifier class is not compatible with the old data format, so please use the new IF data format for the new verifier. The new verifier and the new data will give better results.</p>"},{"location":"tulu3/#llama-31-tulu-3-8b-rm-reproduction","title":"Llama-3.1-Tulu-3-8B-RM Reproduction","text":"<p>This is (almost) the exact command which produced allenai/Llama-3.1-Tulu-3-8B-RM</p> <pre><code>accelerate launch \\\n    --config_file configs/ds_configs/deepspeed_zero3.yaml open_instruct/reward_modeling.py \\\n    --dataset_mixer '{\"allenai/llama-3.1-tulu-3-8b-preference-mixture\": 1.0}' \\\n    --dataset_train_splits train \\\n    --dataset_eval_mixer '{\"allenai/ultrafeedback_binarized_cleaned\": 1.0}' \\\n    --dataset_eval_splits test_prefs \\\n    --model_name_or_path allenai/Llama-3.1-Tulu-3-8B-SFT \\\n    --chat_template tulu \\\n    --learning_rate 3e-6 \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 32 \\\n    --max_token_length 2048 \\\n    --max_prompt_token_length 2048 \\\n    --num_train_epochs 1 \\\n    --output_dir output/rm_8b \\\n    --gradient_checkpointing \\\n    --push_to_hub \\\n    --with_tracking\n# For Ai2 internal members, this was the experiment URL: https://beaker.org/ex/01JCS01RFBQGFE5F1W3W96FFVM/\n</code></pre>"},{"location":"tulu3/#llama-31-tulu-3-8b-reproduction","title":"Llama-3.1-Tulu-3-8B Reproduction","text":"<p>This is (almost) the exact command which produced allenai/Llama-3.1-Tulu-3-8B</p> <pre><code>python open_instruct/ppo_vllm_thread_ray_gtrl.py \\\n    --exp_name tulu-3-8b-rlvr \\\n    --dataset_mixer_list allenai/RLVR-GSM-MATH-IF-Mixed-Constraints 1.0 \\\n    --dataset_mixer_list_splits train \\\n    --dataset_mixer_eval_list allenai/RLVR-GSM-MATH-IF-Mixed-Constraints 16 \\\n    --dataset_mixer_eval_list_splits train \\\n    --max_token_length 2048 \\\n    --max_prompt_token_length 2048 \\\n    --response_length 2048 \\\n    --model_name_or_path allenai/Llama-3.1-Tulu-3-8B-DPO \\\n    --reward_model_path allenai/Llama-3.1-Tulu-3-8B-RM \\\n    --non_stop_penalty \\\n    --stop_token eos \\\n    --temperature 1.0 \\\n    --chat_template_name tulu \\\n    --learning_rate 3e-7 \\\n    --total_episodes 10000000 \\\n    --penalty_reward_value -10.0 \\\n    --deepspeed_stage 3 \\\n    --per_device_train_batch_size 2 \\\n    --local_rollout_forward_batch_size 2 \\\n    --local_mini_batch_size 32 \\\n    --local_rollout_batch_size 32 \\\n    --actor_num_gpus_per_node 7 \\\n    --vllm_tensor_parallel_size 1 \\\n    --beta 0.05 \\\n    --apply_verifiable_reward true \\\n    --output_dir output/rlvr_8b \\\n    --seed 3 \\\n    --num_evals 3 \\\n    --save_freq 100 \\\n    --reward_model_multiplier 0.0 \\\n    --gradient_checkpointing \\\n    --with_tracking\n# For Ai2 internal members, this was the experiment URL: https://beaker.org/ex/01JCVTA10BQDVGGQKFYWEZ6KCQ/\n</code></pre>"},{"location":"tulu3/#llama-31-tulu-3-70b-reproduction","title":"Llama-3.1-Tulu-3-70B Reproduction","text":"<p>This is (almost) the exact command which produced allenai/Llama-3.1-Tulu-3-70B</p> <p>Couple of notes: * Make sure to modify <code>configs/beaker_configs/ray_node_setup.sh</code> in our own cluster setup. The idea is to have the replicas join the main machines via <code>ray</code>. * We had to use <code>--vllm_tensor_parallel_size 4</code> because <code>--vllm_tensor_parallel_size 8</code> errors out for some strange reason. This is a temporary workaround. * Here the effective batch size is <code>sum(actor_num_gpus_per_node) * local_mini_batch_size = 40 * 16 = 640</code>. If you have less GPUs, you can adjust <code>actor_num_gpus_per_node</code> and <code>local_mini_batch_size</code> accordingly.</p> <pre><code>source configs/beaker_configs/ray_node_setup.sh &amp;&amp; python open_instruct/ppo_vllm_thread_ray_gtrl.py \\\n    --dataset_mixer_list allenai/RLVR-GSM-MATH-IF-Mixed-Constraints 1.0 \\\n    --dataset_mixer_list_splits train \\\n    --dataset_mixer_eval_list allenai/RLVR-GSM-MATH-IF-Mixed-Constraints 16 \\\n    --dataset_mixer_eval_list_splits train \\\n    --max_token_length 2048 \\\n    --max_prompt_token_length 2048 \\\n    --response_length 2048 \\\n    --model_name_or_path allenai/Llama-3.1-Tulu-3-70B-DPO \\\n    --exp_name tulu-3-70b-rlvr \\\n    --reward_model_path allenai/Llama-3.1-Tulu-3-8B-RM \\\n    --beta 0.07 \\\n    --warmup_ratio 0.1 \\\n    --seed 8 \\\n    --output_dir output/rlvr_70b \\\n    --non_stop_penalty \\\n    --stop_token eos \\\n    --temperature 1.0 \\\n    --chat_template_name tulu \\\n    --learning_rate 1e-7 \\\n    --total_episodes 400000 \\\n    --penalty_reward_value -10.0 \\\n    --deepspeed_stage 3 \\\n    --per_device_train_batch_size 1 \\\n    --local_rollout_forward_batch_size 1 \\\n    --local_mini_batch_size 16 \\\n    --local_rollout_batch_size 16 \\\n    --actor_num_gpus_per_node 8 8 8 8 8 \\\n    --vllm_num_engines 1 \\\n    --vllm_tensor_parallel_size 4 \\\n    --apply_verifiable_reward true \\\n    --reward_model_multiplier 0.0 \\\n    --no_gather_whole_model \\\n    --num_evals 3 \\\n    --save_freq 40 \\\n    --gradient_checkpointing \\\n    --with_tracking\n# For Ai2 internal members, this was the experiment URL: https://beaker.org/ex/01JD3YEM4XGH2F2H10Y49GK441/\n</code></pre>"},{"location":"tulu3/#llama-31-tulu-3-405b-reproduction","title":"Llama-3.1-Tulu-3-405B Reproduction","text":"<p>This is (almost) the exact command which produced allenai/Llama-3.1-Tulu-3-405B</p> <p>Couple of notes: * We had to set <code>TORCH_NCCL_ENABLE_MONITORING=0</code> to turn off NCCL heartbeat monitoring and avoid timeouts. Feel free to remove this. * Make sure to modify <code>configs/beaker_configs/ray_node_setup.sh</code> in our own cluster setup. The idea is to have the replicas join the main machines via <code>ray</code>. * Here the effective batch size is <code>sum(actor_num_gpus_per_node) * local_mini_batch_size = 40 * 16 = 640</code>. If you have less GPUs, you can adjust <code>actor_num_gpus_per_node</code> and <code>local_mini_batch_size</code> accordingly.</p> <pre><code>TORCH_NCCL_ENABLE_MONITORING=0 python mason.py \\\n    --cluster ai2/jupiter --pure_docker_mode \\\n    --workspace ai2/tulu-3-dev \\\n    --priority urgent \\\n    --preemptible \\\n    --num_nodes 32 \\\n    --image nathanl/open_instruct_auto \\\n    --budget ai2/oe-adapt \\\n    --gpus 8 -- source configs/beaker_configs/ray_node_setup.sh \\&amp;\\&amp; TORCH_DISTRIBUTED_DEBUG=DETAIL python open_instruct/ppo_vllm_thread_ray_gtrl.py \\\n    --dataset_mixer_list allenai/RLVR-MATH 1.0 \\\n    --dataset_mixer_list_splits train \\\n    --dataset_mixer_eval_list allenai/RLVR-MATH 128 \\\n    --dataset_mixer_eval_list_splits train \\\n    --max_token_length 2048 \\\n    --max_prompt_token_length 2048 \\\n    --response_length 1024 \\\n    --model_name_or_path /weka/oe-adapt-default/hamishi/405b_dpo_v4 \\\n    --exp_name \"405b_rlvr_math_only_8b_valu_on_v4\" \\\n    --reward_model_path allenai/Llama-3.1-Tulu-3-8B-RM \\\n    --beta 0.05 \\\n    --output_dir \"/weka/oe-adapt-default/hamishi/405b_rlvr_math_only_8b_valu_on_v4\" \\\n    --non_stop_penalty \\\n    --stop_token eos \\\n    --temperature 1.0 \\\n    --chat_template tulu \\\n    --learning_rate 1e-7 \\\n    --total_episodes 400000 \\\n    --num_epochs 4 \\\n    --penalty_reward_value -10.0 \\\n    --deepspeed_stage 3 \\\n    --per_device_train_batch_size 1 \\\n    --local_rollout_forward_batch_size 1 \\\n    --local_mini_batch_size 8 \\\n    --local_rollout_batch_size 8 \\\n    --actor_num_gpus_per_node 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 \\\n    --vllm_num_engines 1 \\\n    --vllm_tensor_parallel_size 16 \\\n    --vllm_enforce_eager true \\\n    --apply_verifiable_reward true \\\n    --reward_model_multiplier 0.0 \\\n    --no_gather_whole_model \\\n    --seed 3 \\\n    --num_evals 3 \\\n    --no_try_launch_beaker_eval_jobs \\\n    --save_freq 25 \\\n    --try_launch_beaker_eval_jobs_on_weka \\\n    --gradient_checkpointing \\\n    --with_tracking\n# For Ai2 internal members, this was the experiment URL: https://beaker.org/ex/01JJA31S20XAFR82YPFKSMMYZV/\n</code></pre>"},{"location":"tulu3/#new-llama-31-tulu-31-8b-reproduction","title":"(NEW) Llama-3.1-Tulu-3.1-8B Reproduction","text":"<p>This is the exact command which produced allenai/Llama-3.1-Tulu-3.1-8B, which uses 2 nodes (16 GPUs)</p> <pre><code>for learning_rate in 5e-7; do\nfor beta in 0.01; do\nfor nspp in 16; do\nfor m in half-m ; do\nfor kl_estimator in kl3; do\nlocal_rollout_batch_size=4\nif [ $m == \"half-m\" ]; then\n    local_mini_batch_size=$(($local_rollout_batch_size * $nspp / 2))\nelse\n    local_mini_batch_size=$(($local_rollout_batch_size * $nspp))\nfi\nexp_name=\"0204_lr_scan_grpo_math_lr_${learning_rate}_${kl_estimator}_${beta}_${nspp}_${m}_${RANDOM}\"\nfull_bsz=$(($local_rollout_batch_size * nspp * (7) * 2))\necho $exp_name:\necho --- local_mini_batch_size=$local_mini_batch_size\necho --- full_bsz=$full_bsz\necho --- num_gradient_updates=$(($local_rollout_batch_size * $nspp / $local_mini_batch_size))\npython mason.py \\\n    --cluster ai2/jupiter \\\n    --workspace ai2/tulu-3-dev \\\n    --priority high \\\n    --preemptible \\\n    --num_nodes 2 \\\n    --max_retries 1 \\\n    --budget ai2/oe-adapt \\\n    --gpus 8 -- source configs/beaker_configs/ray_node_setup.sh \\&amp;\\&amp; uv run python open_instruct/grpo_vllm_thread_ray_gtrl.py \\\n    --exp_name $exp_name \\\n    --beta $beta \\\n    --local_mini_batch_size $local_mini_batch_size \\\n    --number_samples_per_prompt $nspp \\\n    --output_dir /weka/oe-adapt-default/costah/models/$exp_name \\\n    --local_rollout_batch_size $local_rollout_batch_size \\\n    --kl_estimator $kl_estimator \\\n    --learning_rate $learning_rate \\\n    --dataset_mixer_list allenai/RLVR-GSM-MATH-IF-Mixed-Constraints 1.0 \\\n    --dataset_mixer_list_splits train \\\n    --dataset_mixer_eval_list allenai/RLVR-GSM-MATH-IF-Mixed-Constraints 16 \\\n    --dataset_mixer_eval_list_splits train \\\n    --max_token_length 2048 \\\n    --max_prompt_token_length 2048 \\\n    --response_length 2048 \\\n    --model_name_or_path allenai/Llama-3.1-Tulu-3-8B-DPO \\\n    --non_stop_penalty \\\n    --stop_token eos \\\n    --temperature 1.0 \\\n    --chat_template_name tulu \\\n    --total_episodes 10000000 \\\n    --penalty_reward_value 0.0 \\\n    --deepspeed_stage 2 \\\n    --per_device_train_batch_size 2 \\\n    --local_rollout_forward_batch_size 2 \\\n    --actor_num_gpus_per_node 4 8 \\\n    --num_epochs 1 \\\n    --vllm_tensor_parallel_size 4 \\\n    --lr_scheduler_type constant \\\n    --apply_verifiable_reward true \\\n    --seed 1 \\\n    --num_evals 30 \\\n    --save_freq 40 \\\n    --reward_model_multiplier 0.0 \\\n    --no_try_launch_beaker_eval_jobs \\\n    --try_launch_beaker_eval_jobs_on_weka \\\n    --gradient_checkpointing \\\n    --with_tracking\ndone\ndone\ndone\ndone\ndone\n# For Ai2 internal members, this was the experiment URL: https://beaker.allen.ai/orgs/ai2/workspaces/tulu-3-dev/work/01JKA7CSDGG3YA84X89C5HJPXR?taskId=01JKA7CSDQMVBDNAWF5T7ZXDSA&amp;jobId=01JKH4KYJTR2Y2NYNCCQ63ZQHE\n</code></pre> <p>If you are running on a single node (8 GPUs), consider adjusting the commands as follows. Basically, the idea is to simulate the same batch size. In the two nodes setup, we used <code>--actor_num_gpus_per_node 4 8</code> (12 GPUs) for training, so we multiply it with <code>local_rollout_batch_size=4</code> to get the rollout batch size <code>12 * 4 = 48</code>. Now assume we used <code>--actor_num_gpus_per_node 6</code> (6 GPUs) for training, so we get <code>48 / 6 = 8</code>, which is the new <code>local_rollout_batch_size</code>.</p> <pre><code> for learning_rate in 5e-7; do\n for beta in 0.01; do\n for nspp in 16; do\n for m in half-m ; do\n for kl_estimator in kl3; do\n-local_rollout_batch_size=4\n+local_rollout_batch_size=8\n if [ $m == \"half-m\" ]; then\n     local_mini_batch_size=$(($local_rollout_batch_size * $nspp / 2))\n else\n     local_mini_batch_size=$(($local_rollout_batch_size * $nspp))\n fi\n exp_name=\"0204_lr_scan_grpo_math_lr_${learning_rate}_${kl_estimator}_${beta}_${nspp}_${m}_${RANDOM}\"\n full_bsz=$(($local_rollout_batch_size * nspp * (7) * 2))\n echo $exp_name:\n echo --- local_mini_batch_size=$local_mini_batch_size\n echo --- full_bsz=$full_bsz\n echo --- num_gradient_updates=$(($local_rollout_batch_size * $nspp / $local_mini_batch_size))\n python mason.py \\\n     --cluster ai2/jupiter \\\n     --workspace ai2/tulu-3-dev \\\n     --priority high \\\n     --preemptible \\\n     --num_nodes 2 \\\n     --max_retries 1 \\\n     --budget ai2/oe-adapt \\\n     --gpus 8 -- source configs/beaker_configs/ray_node_setup.sh \\&amp;\\&amp; uv run python open_instruct/grpo_vllm_thread_ray_gtrl.py \\\n     --exp_name $exp_name \\\n     --beta $beta \\\n     --local_mini_batch_size $local_mini_batch_size \\\n     --number_samples_per_prompt $nspp \\\n     --output_dir /weka/oe-adapt-default/costah/models/$exp_name \\\n     --local_rollout_batch_size $local_rollout_batch_size \\\n     --kl_estimator $kl_estimator \\\n     --learning_rate $learning_rate \\\n     --dataset_mixer_list allenai/RLVR-GSM-MATH-IF-Mixed-Constraints 1.0 \\\n     --dataset_mixer_list_splits train \\\n     --dataset_mixer_eval_list allenai/RLVR-GSM-MATH-IF-Mixed-Constraints 16 \\\n     --dataset_mixer_eval_list_splits train \\\n     --max_token_length 2048 \\\n     --max_prompt_token_length 2048 \\\n     --response_length 2048 \\\n     --model_name_or_path allenai/Llama-3.1-Tulu-3-8B-DPO \\\n     --non_stop_penalty \\\n     --stop_token eos \\\n     --temperature 1.0 \\\n     --chat_template_name tulu \\\n     --total_episodes 10000000 \\\n     --penalty_reward_value 0.0 \\\n-    --deepspeed_stage 2 \\\n+    --deepspeed_stage 3 \\\n     --per_device_train_batch_size 2 \\\n     --local_rollout_forward_batch_size 2 \\\n-    --actor_num_gpus_per_node 4 8 \\\n+    --actor_num_gpus_per_node 6 \\\n     --num_epochs 1 \\\n-    --vllm_tensor_parallel_size 4 \\\n+    --vllm_tensor_parallel_size 2 \\\n     --lr_scheduler_type constant \\\n     --apply_verifiable_reward true \\\n     --seed 1 \\\n     --num_evals 30 \\\n     --save_freq 40 \\\n     --reward_model_multiplier 0.0 \\\n     --no_try_launch_beaker_eval_jobs \\\n     --try_launch_beaker_eval_jobs_on_weka \\\n     --gradient_checkpointing \\\n     --with_tracking\n done\n done\n done\n done\n done\n</code></pre>"},{"location":"zero3_explained/","title":"ZeRO-3 Explained","text":""},{"location":"zero3_explained/#the-problem-zero-3-solves","title":"The problem ZeRO-3 solves","text":"<p>A 7B parameter model in fp32 needs ~28 GB just for parameters. But training also needs: - Optimizer states (Adam has 2 states per param): ~56 GB - Gradients: ~28 GB - Total: ~112 GB just for training state, before activations</p> <p>That doesn't fit on one 95 GB GH200. And if you just copy everything to both GPUs (standard data parallelism), each GPU still needs 112 GB.</p>"},{"location":"zero3_explained/#what-zero-3-does","title":"What ZeRO-3 does","text":"<p>Split everything equally. GPU 0 \"owns\" the first half of every layer's parameters, optimizer states, and gradients. GPU 1 owns the second half.</p> <p>At rest, each GPU only stores ~56 GB of training state. It fits.</p>"},{"location":"zero3_explained/#forward-pass-one-layer-at-a-time","title":"Forward pass, one layer at a time","text":"<p>Say we're computing layer 5. Here's what happens:</p> <p>Step 1: All-gather parameters. GPU 0 has params[0:half] for layer 5. GPU 1 has params[half:end] for layer 5. They each send their half to the other. Now both GPUs have the complete layer 5 parameters. This takes network bandwidth.</p> <p>Step 2: Compute locally. GPU 0 runs layer 5 on its own batch of data (say, 4 training examples). GPU 1 runs layer 5 on its own different batch of data (4 different examples). They do this independently, no communication. Each produces its own activations.</p> <p>Step 3: Discard the borrowed half. GPU 0 throws away GPU 1's half of layer 5 params. GPU 1 throws away GPU 0's half. Memory freed.</p> <p>Repeat for layer 6, 7, 8, ...</p>"},{"location":"zero3_explained/#backward-pass-same-idea-but-in-reverse","title":"Backward pass, same idea but in reverse","text":"<p>Going backward through layer 5:</p> <p>Step 1: All-gather parameters again (need them to compute gradients).</p> <p>Step 2: Compute gradients locally. Each GPU computes gradients for its own batch.</p> <p>Step 3: Reduce-scatter gradients. Instead of each GPU keeping full gradients, they split them up: GPU 0 gets the sum of both GPUs' gradients for the first half of params, GPU 1 gets the sum for the second half. Now each GPU only has gradients for the params it owns.</p> <p>Step 4: Discard borrowed params again.</p>"},{"location":"zero3_explained/#optimizer-step","title":"Optimizer step","text":"<p>Each GPU runs Adam on only the params it owns, using only the gradients it owns, updating only the optimizer states it owns. No communication needed.</p>"},{"location":"zero3_explained/#the-key-insight","title":"The key insight","text":"<p>At no point does an activation leave its GPU. Each GPU processes its own independent mini-batch. The only things that cross the network are parameter chunks (temporarily gathered for computation, then discarded) and gradient chunks (scattered so each GPU only keeps what it needs).</p> <p>It's like a shared textbook in a library: two students each store half the pages at their desk. When either needs to read a chapter, they borrow the missing pages, do their own homework, then return them. They never share their homework answers (activations) \u2014 just the textbook pages (parameters).</p>"},{"location":"zero3_explained/#how-this-maps-to-our-setup","title":"How this maps to our setup","text":"<p>In our GRPO training (2 nodes, OLMo-3-7B):</p> Component Count Role Learner GPUs 2 (1 per node) ZeRO-3 sharded training vLLM GPUs 6 (3 per node) Inference only (no training state) <p>Communication backends: - NCCL: all-gather params + reduce-scatter gradients between the 2 learners - Gloo: broadcast updated weights from learner rank-0 to all 6 vLLM engines (after training steps)</p>"},{"location":"zero3_explained/#comparison-with-other-parallelism-strategies","title":"Comparison with other parallelism strategies","text":"Strategy What's split What crosses the network Memory saving Data parallelism Data only (model fully replicated) Gradients (all-reduce) None ZeRO-1 Optimizer states Gradients (all-reduce) ~4x ZeRO-2 Optimizer states + gradients Gradients (reduce-scatter) + params (all-gather only if needed) ~8x ZeRO-3 Optimizer states + gradients + parameters Params (all-gather per layer) + gradients (reduce-scatter) ~Nx (N = num GPUs) Tensor parallelism Individual matrix multiplies Activations (mid-layer) Proportional to split Pipeline parallelism Layers across GPUs Activations (between stages) Proportional to stages <p>ZeRO-3 trades network bandwidth for memory. The all-gather happens for every layer in both forward and backward, so it's communication-heavy, but it means each GPU only needs 1/N of the total training state.</p>"},{"location":"data/preference-data/","title":"Current preference datasets","text":"<p>To build all the datasets at once (use this carefully), run: <pre><code>sh scripts/data/preferences/prepare_all.sh\n</code></pre></p>"},{"location":"data/preference-data/#chat","title":"Chat","text":""},{"location":"data/preference-data/#maintained-here","title":"Maintained here","text":"<p>First, older popular datasets. Build these datasets (a subset only) with: <pre><code>python scripts/data/preferences/webgpt.py --push_to_hub --hf_entity=ai2-adapt-dev\npython scripts/data/preferences/hh-harmless.py --push_to_hub --hf_entity=ai2-adapt-dev\npython scripts/data/preferences/hh-helpful.py --push_to_hub --hf_entity=ai2-adapt-dev\n</code></pre> * ai2-adapt-dev/webgpt-binarized * ai2-adapt-dev/hh-rlhf-harmless * ai2-adapt-dev/hh-rlhf-helpful</p> <p>Next, Nvidia's recent HelpSteer 2. They are created with: <pre><code>python scripts/data/preferences/helpsteer2.py --push_to_hub --min_score 2.5 --hf_entity=ai2-adapt-dev\npython scripts/data/preferences/helpsteer2.py --push_to_hub --min_score 2 --hf_entity=ai2-adapt-dev\npython scripts/data/preferences/helpsteer2.py --push_to_hub --min_score 2 --hf_entity=ai2-adapt-dev --aspects_to_ignore verbosity\npython scripts/data/preferences/helpsteer2.py --push_to_hub --hf_entity=ai2-adapt-dev --aspects_to_ignore verbosity\n</code></pre> The binarization weighting that Nvidia recommends can be used with: <pre><code>python scripts/data/preferences/helpsteer2_nvidia.py --push_to_hub --hf_entity ai2-adapt-dev\n</code></pre> Some examples include: * ai2-adapt-dev/helpsteer-2-binarized-above-2.0-margin-0.5-ignore-verbosity * ai2-adapt-dev/helpsteer-2-binarized-ignore-verbosity: This ignores verbosity aspect, which is unclear in the paper. * ai2-adapt-dev/helpsteer2-binarized-nvidia-spec: This uses the specific weighting that Nvidia converged on in their HelpSteer2 paper training multiple types of reward models.</p> <p>Also, specific splits of Nectar (randomly binarized from top 3 completions and a bottom completion) are included with: <pre><code>python scripts/data/preferences/nectar.py --push_to_hub --hf_entity ai2-adapt-dev\npython scripts/data/preferences/nectar.py --push_to_hub --subset anthropic-hh --hf_entity ai2-adapt-dev\npython scripts/data/preferences/nectar.py --push_to_hub --deduplication --hf_entity ai2-adapt-dev\n</code></pre> The default split is <code>lmsys-chat-1m</code>. The last example is called \"deduplication\" due to potential overlap with UltraFeedback, given they source from the same underlying dataset. Basic tests showed they did not use the same prompts, but slight modifications could've occured. * ai2-adapt-dev/nectar_binarized-anthropic-hh * ai2-adapt-dev/nectar_binarized-lmsys-chat-1m * ai2-adapt-dev/nectar_binarized-dedup-ultrafeedbackack</p>"},{"location":"data/preference-data/#stored-on-hf","title":"Stored on HF","text":"<ul> <li>allenai/ultrafeedback_binarized_cleaned_train</li> <li>ai2-adapt-dev/summarize_from_feedback</li> <li>ai2-adapt-dev/DaringAnteater-prefs</li> <li>ai2-adapt-dev/DaringAnteater-prefs-RM-filter</li> <li>ai2-adapt-dev/WildChat-prefs-280824</li> <li>ai2-adapt-dev/helpsteer2-binarized-mean-aspects: Similar to our other HelpSteer splits, less processing.</li> <li>ai2-adapt-dev/Skywork-Magpie: Subset of the Skywork Preference Dataset for only the Magpie splits.</li> </ul>"},{"location":"data/preference-data/#ultrafeedback-replication","title":"UltraFeedback Replication","text":"<p>The current replications have fewer prompts than the original. These are built by splitting the original and recreating completions. We are working on merging them.</p> <p>Build these datasets with: <pre><code>python scripts/data/preferences/ultrafeedback.py --push_to_hub --hf_entity=ai2-adapt-dev\n</code></pre> The master version of the UltraFeedback pipeline replication can be found here: ai2-adapt-dev/ultrafeedback-pipeline-replication</p> <p>UltraFeedback variants explore different combinations of prompt sources, model diversity, sampling methods, and prompt templates:</p> <ul> <li>Setup 0: Replication of original UltraFeedback</li> <li>Setup 1-2: Custom prompts with UltraFeedback methodology</li> <li>Setup 3-4: Custom prompts with varied model diversity and principle sampling</li> <li>Setup 5: Custom prompts with UltraFeedback template</li> <li> <p>Setup 6: Increased model diversity</p> </li> <li> <p>ai2-adapt-dev/ultrafeedback-replication-p0</p> </li> <li>ai2-adapt-dev/ultrafeedback-replication-p1</li> <li>ai2-adapt-dev/ultrafeedback-replication-p2</li> <li>ai2-adapt-dev/ultrafeedback-replication-p3</li> <li>ai2-adapt-dev/ultrafeedback-replication-p4</li> <li>ai2-adapt-dev/ultrafeedback-replication-p5</li> <li>ai2-adapt-dev/ultrafeedback-replication-p6</li> </ul>"},{"location":"data/preference-data/#ultrainteract-variants","title":"UltraInteract Variants","text":"<p>Build these datasets with: <pre><code>python scripts/data/preferences/ultrainteract.py --push_to_hub --hf_entity=ai2-adapt-dev\n</code></pre> Split by category and by selecting the longest conversations per prompt or a random length per prompt. From UltraInteract_pair.</p> <ul> <li>ai2-adapt-dev/UltraInteract_pair_maxlen_Coding</li> <li>ai2-adapt-dev/UltraInteract_pair_randomlen_Coding</li> <li>ai2-adapt-dev/UltraInteract_pair_maxlen_Math_CoT</li> <li>ai2-adapt-dev/UltraInteract_pair_randomlen_Math_CoT</li> <li>ai2-adapt-dev/UltraInteract_pair_maxlen_Math_PoT</li> <li>ai2-adapt-dev/UltraInteract_pair_randomlen_Math_PoT</li> <li>ai2-adapt-dev/UltraInteract_pair_maxlen_Logic</li> <li>ai2-adapt-dev/UltraInteract_pair_randomlen_Logic</li> </ul>"},{"location":"data/preference-data/#tulu-25-data","title":"Tulu 2.5 Data","text":"<p>Build these datasets with: <pre><code>python scripts/data/preferences/split_tulu2.5_prefs.py --push_to_hub --hf_entity=ai2-adapt-dev\n</code></pre> Split from this dataset for easier mixing: * ai2-adapt-dev/tulu-2.5-prefs-alpaca_farm_gpt4_pref * ai2-adapt-dev/tulu-2.5-prefs-alpaca_farm_human_pref * ai2-adapt-dev/tulu-2.5-prefs-argilla_dpo_mix * ai2-adapt-dev/tulu-2.5-prefs-capybara * ai2-adapt-dev/tulu-2.5-prefs-chatbot_arena_2023 * ai2-adapt-dev/tulu-2.5-prefs-chatbot_arena_2024 * ai2-adapt-dev/tulu-2.5-prefs-helpsteer * ai2-adapt-dev/tulu-2.5-prefs-hh_rlhf * ai2-adapt-dev/tulu-2.5-prefs-hh_rlhf_60k * ai2-adapt-dev/tulu-2.5-prefs-nectar * ai2-adapt-dev/tulu-2.5-prefs-nectar_60k * ai2-adapt-dev/tulu-2.5-prefs-orca_dpo_pairs * ai2-adapt-dev/tulu-2.5-prefs-preference_big_mixture * ai2-adapt-dev/tulu-2.5-prefs-prm800k_pairs_phase2 * ai2-adapt-dev/tulu-2.5-prefs-shp_2 * ai2-adapt-dev/tulu-2.5-prefs-stack_exchange_60k * ai2-adapt-dev/tulu-2.5-prefs-stack_exchange_paired * ai2-adapt-dev/tulu-2.5-prefs-ultrafeedback_evol_instruct * ai2-adapt-dev/tulu-2.5-prefs-ultrafeedback_false_qa * ai2-adapt-dev/tulu-2.5-prefs-ultrafeedback_flan_v2 * ai2-adapt-dev/tulu-2.5-prefs-ultrafeedback_lower_10k * ai2-adapt-dev/tulu-2.5-prefs-ultrafeedback_mean_aspects * ai2-adapt-dev/tulu-2.5-prefs-ultrafeedback_middle_10k * ai2-adapt-dev/tulu-2.5-prefs-ultrafeedback_overall * ai2-adapt-dev/tulu-2.5-prefs-ultrafeedback_sharegpt * ai2-adapt-dev/tulu-2.5-prefs-ultrafeedback_top_10k * ai2-adapt-dev/tulu-2.5-prefs-ultrafeedback_truthful_qa * ai2-adapt-dev/tulu-2.5-prefs-ultrafeedback_ultrachat</p>"},{"location":"get_started/ai2_internal_setup/","title":"Ai2 Internal Setup","text":"<p>This document details some best practices when working with our cluster.</p>"},{"location":"get_started/ai2_internal_setup/#one-time-setup-vscode-weka-setup","title":"(One-time setup) VScode + Weka setup","text":"<p>You should join the <code>#vscode-weka-dev-workflow</code> slack channel to setup your VScode to work with weka.</p> <p>After following the instructions there, you should end up with a VScode / Cursor setup that looks like this:</p> <ul> <li>Your terminal has direct access to the weka filesystem.</li> <li>You can run <code>beaker</code> commands from the terminal.</li> <li>You can edit files in the weka filesystem.</li> <li>You can run python scripts with the pyenv / uv environment.</li> </ul> <p></p>"},{"location":"get_started/ai2_internal_setup/#one-time-setup-setup-api-keys","title":"(One-time setup) Setup API keys","text":"<p>You need to first obtain API key or tokens from the following website:</p> <ul> <li><code>BEAKER_TOKEN</code>: https://beaker.org/user</li> <li><code>WANDB_API_KEY</code>: https://wandb.ai/authorize</li> <li><code>HF_TOKEN</code>: https://huggingface.co/settings/tokens</li> </ul> <p>Then you need to write them in beaker secret as follows (replace the <code>xxxx</code> with your own API key or token) <pre><code>beaker_whoami=$(beaker account whoami --format json | jq -r '.[0].name')\nbeaker secret write -w ai2/tulu-2-improvements \"${beaker_whoami}_BEAKER_TOKEN\" xxxx\nbeaker secret write -w ai2/tulu-2-improvements \"${beaker_whoami}_WANDB_API_KEY\" xxxx\nbeaker secret write -w ai2/tulu-2-improvements \"${beaker_whoami}_HF_TOKEN\" xxxx\nbeaker secret write -w ai2/tulu-3-dev \"${beaker_whoami}_BEAKER_TOKEN\" xxxx\nbeaker secret write -w ai2/tulu-3-dev \"${beaker_whoami}_WANDB_API_KEY\" xxxx\nbeaker secret write -w ai2/tulu-3-dev \"${beaker_whoami}_HF_TOKEN\" xxxx\n</code></pre></p>"},{"location":"get_started/ai2_internal_setup/#masonpy-for-job-submission","title":"mason.py (for job submission)","text":"<p><code>mason.py</code> is our job submission script. It basically takes your command and runs it in the specified clusters.</p> <p>For example, let's say you have a training job like this:</p> <pre><code>python open_instruct/finetune.py \\\n    --model_name_or_path EleutherAI/pythia-14m \\\n    --tokenizer_name EleutherAI/pythia-14m \\\n    --dataset_mixer_list allenai/tulu-3-sft-personas-algebra 100 \\\n    --use_flash_attn False \\\n    --with_tracking --report_to wandb\n</code></pre> <p>You can take your command above and run it on the weka cluster with the following command (use <code>--</code> to separate the mason command from the python command):</p> <pre><code>python mason.py \\\n    --cluster ai2/jupiter ai2/saturn ai2/neptune \\\n    --workspace ai2/tulu-3-dev \\\n    --image nathanl/open_instruct_auto --pure_docker_mode \\\n    --priority normal \\\n    --budget ai2/oe-adapt \\\n    --gpus 0 -- python open_instruct/finetune.py \\\n    --model_name_or_path EleutherAI/pythia-14m \\\n    --tokenizer_name EleutherAI/pythia-14m \\\n    --dataset_mixer_list allenai/tulu-3-sft-personas-algebra 100 \\\n    --use_flash_attn False \\\n    --with_tracking --report_to wandb\n</code></pre> <p></p> <p></p> <p><code>mason.py</code> does a few things:</p> <p>Auto set HF cache environment variables:</p> <p>During the job submission, it automatically tries to setup a shared Hugging Face cache with environment variables. For example, it sets</p> <ul> <li><code>HF_HOME=/weka/oe-adapt-default/allennlp/.cache/huggingface</code>.</li> <li><code>HF_DATASETS_CACHE=/weka/oe-adapt-default/allennlp/.cache/huggingface</code></li> <li><code>HF_HUB_CACHE=/weka/oe-adapt-default/allennlp/.cache/hub</code></li> </ul> <p>Auto set <code>--hf_entity</code> and <code>--wandb_entity</code> arguments:</p> <p>so during runtime we issue fewer HF API calls, which sometimes could fail due to rate limiting.</p> <p>Auto caching datasets:</p> <p>mason.py will auto call <code>--cache_dataset_only</code> for you, so you do the tokenization locally instead of in the jobs, which saves idle GPU time in the actual jobs.</p> <p>Auto upload to Google Cloud Storage:</p> <p>When submitting to the <code>ai2/augusta</code> cluster, mason will try to read your model and upload it to Google Cloud Storage and download it to the job (since the cluster does not have a reliable shared filesystem).</p>"},{"location":"get_started/ai2_internal_setup/#update_command_argspy-for-sweep-benchmark-etc","title":"update_command_args.py (for sweep, benchmark, etc.)","text":"<p>The /scripts/train directory contains many examples on how to launch jobs with mason.py. Sometimes the commands can get long and hard to manage, so we wrote a script called update_command_args.py that can be used to add or update arguments in a shell script. For example,</p> <pre><code>python update_command_args.py scripts/train/tulu3/grpo_fast_8b.sh \\\n    --cluster ai2/augusta \\\n    --priority normal \\\n    --image costah/open_instruct_dev0320_11  --non_stop_penalty False | uv run bash\n</code></pre> <p>This will update the <code>--cluster</code>, <code>--priority</code>, <code>--image</code>, and <code>--non_stop_penalty</code> arguments in the script with the ones specified, making it easier to launch jobs with different configurations.</p> <p>As another example, you can run something like this for a learning rate search:</p> <pre><code>for lr in 1e-6 1e-5 1e-4; do\n    python update_command_args.py scripts/train/tulu3/grpo_fast_8b.sh \\\n        --exp_name grpo_fast_8b_lr_${lr} \\\n        --learning_rate $lr \\\n        --image costah/open_instruct_dev0320_11 --non_stop_penalty False | uv run bash\ndone\n</code></pre> <p>We also have a script called scripts/train/benchmark.sh that keeps track of all the commands used to launch jobs in our public wandb project <code>ai2-llm/open_instruct_public</code>.</p>"},{"location":"get_started/ai2_internal_setup/#ai2-internal-evaluation","title":"Ai2 Internal Evaluation","text":"<p>We provide a script integrated with beaker for use internally at Ai2. There are couple of use cases.</p> <p>1. Run evals against a public Hugging Face model. Basically you need to prefix the model name with <code>hf-</code> and provide the location as the HF path (e.g. <code>meta-llama/Meta-Llama-3-8B-Instruct</code>).</p> <pre><code>for model in allenai/OLMoE-1B-7B-0125-Instruct allenai/OLMoE-1B-7B-0125-DPO allenai/OLMoE-1B-7B-0125-SFT allenai/OLMoE-1B-7B-0924-SFT allenai/OLMoE-1B-7B-0924-Instruct; do\npython scripts/submit_eval_jobs.py \\\n    --model_name hf-$model \\\n    --cluster ai2/jupiter ai2/neptune ai2/saturn ai2/ceres  \\\n    --priority high \\\n    --location $model \\\n    --is_tuned \\\n    --workspace \"tulu-3-results\" \\\n    --priority high \\\n    --preemptible \\\n    --use_hf_tokenizer_template \\\n    --run_oe_eval_experiments \\\n    --evaluate_on_weka \\\n    --skip_oi_evals\ndone\n</code></pre> <p>2. Run evals against a model hosted on Beaker dataset. If it's a training run, you should try matching the <code>exp_name</code> and <code>run_id</code> with the training run.</p> <pre><code>model_name=0222_32B_dpo_lr_8.5e-7__allenai_open_instruct_dev__42__1741225304\nurl=https://wandb.ai/ai2-llm/open_instruct_internal/runs/7afq8x28\nlocation=01JNMHSM8DDSFB3GJDBM5MP6J8\npython scripts/submit_eval_jobs.py \\\n    --model_name $model_name \\\n    --cluster ai2/jupiter ai2/neptune ai2/saturn ai2/ceres  \\\n    --priority high \\\n    --location $location \\\n    --is_tuned \\\n    --workspace \"tulu-3-results\" \\\n    --preemptible \\\n    --use_hf_tokenizer_template \\\n    --run_oe_eval_experiments \\\n    --skip_oi_evals \\\n    --run_id $url\n</code></pre> <p>This will later show up in the internal leaderboard.</p> <p></p> <p>3. Run evals against a model hosted on weka.</p> <pre><code>python scripts/submit_eval_jobs.py \\\n    --model_name test_no_hf_upload \\\n    --location /weka/oe-adapt-default/costah/models/0129_grpo_math_kl_fix_zs_0.0_16_half-m_461_checkpoints/step_640 \\\n    --cluster ai2/saturn ai2/neptune \\\n    --is_tuned \\\n    --workspace \"tulu-3-results\" \\\n    --priority high \\\n    --preemptible \\\n    --use_hf_tokenizer_template \\\n    --beaker_image \"nathanl/open_instruct_auto\" \\\n    --run_oe_eval_experiments \\\n    --evaluate_on_weka \\\n    --oe_eval_tasks gsm8k::tulu,minerva_math::tulu \\\n    --run_id https://wandb.ai/ai2-llm/open_instruct_internal/runs/swf79vby \\\n    --skip_oi_evals \\\n    --oe_eval_max_length 8096\n</code></pre> <p>4. Run evals against a model on Google Cloud Storage.</p> <pre><code>python scripts/submit_eval_jobs.py \\\n    --model_name test_gs_location \\\n    --location gs://ai2-llm/post-training/allenai/Llama-3.1-Tulu-3.1-8B \\\n    --cluster ai2/augusta \\\n    --is_tuned \\\n    --workspace tulu-3-results \\\n    --preemptible \\\n    --use_hf_tokenizer_template \\\n    --beaker_image nathanl/open_instruct_auto \\\n    --oe_eval_tasks gsm8k::tulu \\\n    --skip_oi_evals \\\n    --gpu_multiplier 2 \\\n    --run_oe_eval_experiments\n</code></pre>"},{"location":"get_started/ai2_internal_setup/#running-with-gantry","title":"Running with gantry","text":"<p>You can also run with gantry, if you want to test changes. Important: Before you run any command with gantry, make sure you commit and push, since gantry will attempt to clone the repo with your local latest commit hash.</p> <p>See the \"One-Time Setup\" section below before running commands. To test your setup, run the following command -- if this job succeeds, then you're ready to run evaluations with gantry.</p> <pre><code>gantry run --workspace {workspace} --budget ai2/oe-adapt --beaker-image kavelr/oe-safety --venv base --cluster ai2/jupiter --env-secret OPENAI_API_KEY=openai_api_key --env-secret HF_TOKEN=hf_token -- python -c 'print(\"Hello world\")'\n</code></pre> <p>You can freely add any additional arguments to give to Beaker, such as a <code>--priority</code> tag which can be set to preemptible, normal, high, or urgent. AI2 policies may restrict the priorities that are available to users on certain clusters.</p> <p>In the examples below, text within {} tags should be replaced with your own values.</p> <p>As a convenience, you can use the <code>evaluation/gantry_run.sh</code> script which includes some necessary arguments. You can use it the same way as <code>gantry run</code>, but excluding these boilerplate arguments (take a look at the script to see what it includes). Example usage:</p> <pre><code>PYTHONPATH=safety-eval ./evaluation/gantry_run.sh --workspace {workspace} --cluster {cluster} --gpus {n_gpus} \\\n    --priority {priority} -- python evaluation/run_all_generation_benchmarks.py \\\n    --model_name_or_path allenai/tulu-2-dpo-7b \\\n    --model_input_template_path_or_name tulu2 \\\n    --report_output_path /results/metrics.json\n</code></pre>"},{"location":"get_started/ai2_internal_setup/#extra-beaker-commands","title":"Extra Beaker Commands","text":"<p>Here is an example using the full <code>gantry run</code> command. Use the beaker image <code>seungjuh/oe-safety-support-olmo17</code></p> <p>Important: Please include all the beaker arguments exactly as in the examples unless intentionally modifying some configuration. Many of them are necessary to avoid job failures, such as <code>--beaker-image</code>, <code>--venv</code>, and <code>--env-secret</code>. Note that <code>openai_api_key</code> and <code>hf_token</code> are Beaker workspace secret names, so should not be replaced with actual values (see One-Time Setup).</p> <p>Note that the <code>--</code> divides the gantry command from the evaluation command - you can edit the second part to run whatever eval suite you want from the <code>eval.py</code> script. Any additional Beaker arguments such as a dataset mount to use a model from a Beaker dataset or adding a priority tag can be added before the <code>--</code>.</p> <p>You can also run all generator evaluations parallelized across the GPUs allocated to your batch job, like so: <pre><code>gantry run --workspace {your_workspace} --cluster {cluster} --gpus {n_gpus} \\\n    --name {beaker_experiment_name} --task-name {beaker_task_name} --beaker-image seungjuh/oe-safety-support-olmo17 --venv base \\\n    --env-secret OPENAI_API_KEY=openai_api_key \\\n    --env-secret HF_TOKEN=hf_token \\\n    --budget {budget} -- python evaluation/run_all_generation_benchmarks.py \\\n    --model_name_or_path allenai/tulu-2-dpo-7b \\\n    --model_input_template_path_or_name tulu2 \\\n    --report_output_path /results/metrics.json --save_individual_results_path /results/all.json\n</code></pre></p> <p>Because the <code>--report_output_path</code> argument is set to <code>/results/metrics.json</code>, the output will automatically get logged to Beaker metrics in the experiment page (example).</p>"},{"location":"get_started/ai2_internal_setup/#common-gotchas","title":"Common Gotchas","text":"<p>If you're experiencing job failures, here are some things to check:</p> <ul> <li>Make sure your local changes are committed,  pushed, and up to date with the remote</li> <li>Make sure you have <code>--beaker-image seungjuh/oe-safety-support-olmo17</code> and <code>--venv base</code> in your <code>gantry run</code> command</li> <li>Check your GitHub personal access token is authorized to access the allenai organization</li> <li>Make sure the openai_api_key and hf_token secrets exist in your Beaker workspace</li> </ul>"},{"location":"get_started/installation/","title":"Installation","text":"<p>Our setup mostly follows our Dockerfile, which uses Python 3.10. Note that Open Instruct is a research codebase and does not guarantee backward compatibility. We offer two installation strategies:</p> <ul> <li> <p>Local installation: This is the recommended way to install Open Instruct. You can install the dependencies by running the following commands: <pre><code>pip install --upgrade pip \"setuptools&lt;70.0.0\" wheel\n# TODO, unpin setuptools when this issue in flash attention is resolved\npip install torch==2.5.1 torchvision==0.20.1 --index-url https://download.pytorch.org/whl/cu121\npip install packaging\npip install flash-attn==2.7.2.post1 --no-build-isolation\npip install -r requirements.txt\npip install -e .\npython -m nltk.downloader punkt\n</code></pre></p> </li> <li> <p>Local installation with uv (preview): We are experimenting with using uv. You can install via <pre><code>uv sync\nuv sync --extra compile # to install flash attention\n</code></pre></p> </li> <li> <p>Docker installation: You can also use the Dockerfile to build a Docker image. You can build the image with the following command:</p> </li> </ul> <pre><code>docker build . -t open_instruct_dev\n# if you are interally at AI2, you can create an image like this:\nbeaker_user=$(beaker account whoami --format json | jq -r '.[0].name')\nbeaker image delete $beaker_user/open_instruct_dev\nbeaker image create open_instruct_dev -n open_instruct_dev -w ai2/$beaker_user\n</code></pre> <p>Optionally you can build the base image with the following command:</p> <pre><code>docker build --build-arg CUDA=12.1.0 --build-arg TARGET=cudnn8-devel --build-arg DIST=ubuntu20.04 -f  Dockerfile.base . -t cuda-no-conda:12.1-cudnn8-dev-ubuntu20.04\n</code></pre> <ul> <li>Docker with uv: You can also use the Dockerfile to build a Docker image with uv. You can build the image with the following command:</li> </ul> <pre><code>docker build -f Dockerfile.uv --build-arg UV_CACHE_DIR=$UV_CACHE_DIR -t open_instruct_dev_uv .\n# if you are interally at AI2, you can create an image like this:\nbeaker_user=$(beaker account whoami --format json | jq -r '.[0].name')\nbeaker image delete $beaker_user/open_instruct_dev_uv\nbeaker image create open_instruct_dev_uv -n open_instruct_dev_uv -w ai2/$beaker_user\n</code></pre> <p>If you are internally at AI2, you may launch experiments using our always-up-to-date auto-built image <code>nathanl/open_instruct_auto</code>.</p>"}]}